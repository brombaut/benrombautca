<h1 id="titanic-dataset---will-a-passeger-survive">Titanic Dataset -
Will a Passeger Survive?</h1>
<p>The goal is to predict whether or not a passenger survived based on
attributes such as their age, sex, passenger class, where they embarked
and so on.</p>
<p>The challenge is from Kaggle and can be found at <a
href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a>.</p>
<p>First, let’s load the dataset from the link above and save the
<code>CSV</code> files in <code>./data</code>. We will then load
them.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>TITANIC_PATH <span class="op">=</span> <span class="st">&quot;./data&quot;</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_titanic_data(filename, titanic_path<span class="op">=</span>TITANIC_PATH):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    csv_path <span class="op">=</span> os.path.join(titanic_path, filename)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.read_csv(csv_path)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> load_titanic_data(<span class="st">&quot;train.csv&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> load_titanic_data(<span class="st">&quot;test.csv&quot;</span>)</span></code></pre></div>
<p>The data is already split into a training set and a test set.
However, the test data does not contain the labels: your goal is to
train the best model you can using the training data, then make your
predictions on the test data and upload them to Kaggle to see your final
score.</p>
<p>Let’s take a peek at the top few rows of the training set:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>train_data.head()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
</div>
<p>The attributes have the following meaning:</p>
<ul>
<li><strong>Survived</strong>: that’s the target, 0 means the passenger
did not survive, while 1 means he/she survived.</li>
<li><strong>Pclass</strong>: passenger class.</li>
<li><strong>Name</strong>, <strong>Sex</strong>, <strong>Age</strong>:
self-explanatory</li>
<li><strong>SibSp</strong>: how many siblings &amp; spouses of the
passenger aboard the Titanic.</li>
<li><strong>Parch</strong>: how many children &amp; parents of the
passenger aboard the Titanic.</li>
<li><strong>Ticket</strong>: ticket id</li>
<li><strong>Fare</strong>: price paid (in pounds)</li>
<li><strong>Cabin</strong>: passenger’s cabin number</li>
<li><strong>Embarked</strong>: where the passenger embarked the
Titanic</li>
</ul>
<p>Let’s get more info to see how much data is missing:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>train_data.info()</span></code></pre></div>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB</code></pre>
<p>Okay, the <strong>Age</strong>, <strong>Cabin</strong> and
<strong>Embarked</strong> attributes are sometimes null (less than 891
non-null), especially the <strong>Cabin</strong> (77% are null). We will
ignore the <strong>Cabin</strong> for now and focus on the rest. The
<strong>Age</strong> attribute has about 19% null values, so we will
need to decide what to do with them. Replacing null values with the
median age seems reasonable.</p>
<p>The <strong>Name</strong> and <strong>Ticket</strong> attributes may
have some value, but they will be a bit tricky to convert into useful
numbers that a model can consume. So for now, we will ignore them.</p>
<p>Let’s take a look at the numerical attributes:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_data.describe()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Fare
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
714.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
446.000000
</td>
<td>
0.383838
</td>
<td>
2.308642
</td>
<td>
29.699118
</td>
<td>
0.523008
</td>
<td>
0.381594
</td>
<td>
32.204208
</td>
</tr>
<tr>
<th>
std
</th>
<td>
257.353842
</td>
<td>
0.486592
</td>
<td>
0.836071
</td>
<td>
14.526497
</td>
<td>
1.102743
</td>
<td>
0.806057
</td>
<td>
49.693429
</td>
</tr>
<tr>
<th>
min
</th>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.420000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
223.500000
</td>
<td>
0.000000
</td>
<td>
2.000000
</td>
<td>
20.125000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
7.910400
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
446.000000
</td>
<td>
0.000000
</td>
<td>
3.000000
</td>
<td>
28.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
14.454200
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
668.500000
</td>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
38.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
31.000000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
891.000000
</td>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
80.000000
</td>
<td>
8.000000
</td>
<td>
6.000000
</td>
<td>
512.329200
</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Yikes, only 38% <strong>Survived</strong>. :( That’s close enough to
40%, so accuracy will be a reasonable metric to evaluate our model.</li>
<li>The mean <strong>Fare</strong> was £32.20, which does not seem so
expensive (but it was probably a lot of money back then).</li>
<li>The mean <strong>Age</strong> was less than 30 years old.</li>
</ul>
<p>Let’s check that the target is indeed 0 or 1:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;Survived&quot;</span>].value_counts()</span></code></pre></div>
<pre><code>0    549
1    342
Name: Survived, dtype: int64</code></pre>
<p>Now let’s take a quick look at all the categorical attributes:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;Pclass&quot;</span>].value_counts()</span></code></pre></div>
<pre><code>3    491
1    216
2    184
Name: Pclass, dtype: int64</code></pre>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;Sex&quot;</span>].value_counts()</span></code></pre></div>
<pre><code>male      577
female    314
Name: Sex, dtype: int64</code></pre>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;Embarked&quot;</span>].value_counts()</span></code></pre></div>
<pre><code>S    644
C    168
Q     77
Name: Embarked, dtype: int64</code></pre>
<p>The <strong>Embarked</strong> attribute tells us where the passenger
embarked: C=Cherbourg, Q=Queenstown, S=Southampton.</p>
<p>Now let’s build our preprocessing pipelines:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.base <span class="im">import</span> BaseEstimator, TransformerMixin</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>numeric_features <span class="op">=</span> [<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;SibSp&quot;</span>, <span class="st">&quot;Parch&quot;</span>, <span class="st">&quot;Fare&quot;</span>]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>numeric_transformer <span class="op">=</span> Pipeline(</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span>[</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;imputer&quot;</span>, SimpleImputer(strategy<span class="op">=</span><span class="st">&quot;median&quot;</span>)),</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspired from stackoverflow.com/questions/25239958</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MostFrequentImputer(BaseEstimator, TransformerMixin):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.most_frequent_ <span class="op">=</span> pd.Series([X[c].value_counts().index[<span class="dv">0</span>] <span class="cf">for</span> c <span class="kw">in</span> X],</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>                                        index<span class="op">=</span>X.columns)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> transform(<span class="va">self</span>, X, y<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X.fillna(<span class="va">self</span>.most_frequent_)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>categorical_features <span class="op">=</span> [<span class="st">&quot;Pclass&quot;</span>, <span class="st">&quot;Sex&quot;</span>, <span class="st">&quot;Embarked&quot;</span>]</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>categorical_transformer <span class="op">=</span> Pipeline(</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    steps<span class="op">=</span>[</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;imputer&quot;</span>, MostFrequentImputer()),</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat_encoder&quot;</span>, OneHotEncoder(sparse<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>preprocessor <span class="op">=</span> ColumnTransformer(</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;num&quot;</span>, numeric_transformer, numeric_features),</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        (<span class="st">&quot;cat&quot;</span>, categorical_transformer, categorical_features),</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> preprocessor.fit_transform(train_data)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>X_train</span></code></pre></div>
<pre><code>array([[22.,  1.,  0., ...,  0.,  0.,  1.],
       [38.,  1.,  0., ...,  1.,  0.,  0.],
       [26.,  0.,  0., ...,  0.,  0.,  1.],
       ...,
       [28.,  1.,  2., ...,  0.,  0.,  1.],
       [26.,  0.,  0., ...,  1.,  0.,  0.],
       [32.,  0.,  0., ...,  0.,  1.,  0.]])</code></pre>
<p>Cool! Now we have a nice preprocessing pipeline that takes the raw
data and outputs numerical input features that we can feed to any
Machine Learning model we want.</p>
<p>Now the labels:</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> train_data[<span class="st">&quot;Survived&quot;</span>]</span></code></pre></div>
<p>We are now ready to train a classifier. Let’s start with an
<code>SVC</code>:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>svm_clf <span class="op">=</span> SVC(gamma<span class="op">=</span><span class="st">&quot;auto&quot;</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>svm_clf.fit(X_train, y_train)</span></code></pre></div>
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style>
<div id="sk-container-id-2" class="sk-top-container">
<div class="sk-text-repr-fallback">
<pre>SVC(gamma=&#x27;auto&#x27;)</pre>
<b>In a Jupyter environment, please rerun this cell to show the HTML
representation or trust the notebook. <br />On GitHub, the HTML
representation is unable to render, please try loading this page with
nbviewer.org.</b>
</div>
<div class="sk-container" hidden="">
<div class="sk-item">
<div class="sk-estimator sk-toggleable">
<input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label>
<div class="sk-toggleable__content">
<pre>SVC(gamma=&#x27;auto&#x27;)</pre>
</div>
</div>
</div>
</div>
</div>
<p>Great, our model is trained, let’s use it to make predictions on the
test set:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> preprocessor.transform(test_data)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> svm_clf.predict(X_test)</span></code></pre></div>
<p>And now we could just build a CSV file with these predictions
(respecting the format accepted by Kaggle), then upload it and hope for
the best. But first, let’s use cross-validation to have an idea of how
good our model is.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>svm_scores <span class="op">=</span> cross_val_score(svm_clf, X_train, y_train, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>svm_scores.mean()</span></code></pre></div>
<pre><code>0.7329588014981274</code></pre>
<p>Ok, over 73% accuracy, which is clearly better than random chance,
but it’s not a great score. Let’s try to get at least 80% accuracy.</p>
<p>Let’s try a <code>RandomForestClassifier</code>:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>forest_clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>forest_scores <span class="op">=</span> cross_val_score(forest_clf, X_train, y_train, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>forest_scores.mean()</span></code></pre></div>
<pre><code>0.8126466916354558</code></pre>
<p>That’s much better!</p>
<p>Instead of just looking at the mean accuracy across the 10
cross-validation folds, let’s plot all 10 scores for each model, along
with a box plot highlighting the lower and upper quartiles, and
“whiskers” showing the extent of the scores.</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">1</span>]<span class="op">*</span><span class="dv">10</span>, svm_scores, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">2</span>]<span class="op">*</span><span class="dv">10</span>, forest_scores, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>plt.boxplot([svm_scores, forest_scores], labels<span class="op">=</span>(<span class="st">&quot;SVM&quot;</span>,<span class="st">&quot;Random Forest&quot;</span>))</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Accuracy&quot;</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img
src="https://raw.githubusercontent.com/brombaut/benrombautca/main/src/articles/content/sources_notebooks/images/titanic_dataset/titanic_dataset_31_0.png" /></p>
<p>To improve this result further, you could:</p>
<ul>
<li>Compare many more models and tune hyperparameters using cross
validation and grid search,</li>
<li>Do more feature engineering, for example:
<ul>
<li>replace <strong>SibSp</strong> and <strong>Parch</strong> with their
sum,</li>
<li>try to identify parts of names that correlate well with the
<strong>Survived</strong> attribute (e.g. if the name contains
“Countess”, then survival seems more likely),</li>
</ul></li>
<li>try to convert numerical attributes to categorical attributes: for
example, different age groups had very different survival rates (see
below), so it may help to create an age bucket category and use it
instead of the age. Similarly, it may be useful to have a special
category for people traveling alone since only 30% of them survived (see
below).</li>
</ul>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;AgeBucket&quot;</span>] <span class="op">=</span> train_data[<span class="st">&quot;Age&quot;</span>] <span class="op">//</span> <span class="dv">15</span> <span class="op">*</span> <span class="dv">15</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>train_data[[<span class="st">&quot;AgeBucket&quot;</span>, <span class="st">&quot;Survived&quot;</span>]].groupby([<span class="st">&#39;AgeBucket&#39;</span>]).mean()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Survived
</th>
</tr>
<tr>
<th>
AgeBucket
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0.0
</th>
<td>
0.576923
</td>
</tr>
<tr>
<th>
15.0
</th>
<td>
0.362745
</td>
</tr>
<tr>
<th>
30.0
</th>
<td>
0.423256
</td>
</tr>
<tr>
<th>
45.0
</th>
<td>
0.404494
</td>
</tr>
<tr>
<th>
60.0
</th>
<td>
0.240000
</td>
</tr>
<tr>
<th>
75.0
</th>
<td>
1.000000
</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>train_data[<span class="st">&quot;RelativesOnboard&quot;</span>] <span class="op">=</span> train_data[<span class="st">&quot;SibSp&quot;</span>] <span class="op">+</span> train_data[<span class="st">&quot;Parch&quot;</span>]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>train_data[[<span class="st">&quot;RelativesOnboard&quot;</span>, <span class="st">&quot;Survived&quot;</span>]].groupby([<span class="st">&#39;RelativesOnboard&#39;</span>]).mean()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Survived
</th>
</tr>
<tr>
<th>
RelativesOnboard
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.303538
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0.552795
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.578431
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.724138
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.200000
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0.136364
</td>
</tr>
<tr>
<th>
6
</th>
<td>
0.333333
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0.000000
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0.000000
</td>
</tr>
</tbody>
</table>
</div>
