[
  {
    "_id": "deploy_ghpages_actions",
    "_body": "<h2 id=\"create-github-actions-yaml-file.\">Create GitHub Actions YAML file.</h2>\n<p>Will use James Ives‚Äô github-pages-deploy-action</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode yml\"><code class=\"sourceCode yaml\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"co\"># .github/workflows/gh_pages_deploy.yml</span></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">name</span><span class=\"kw\">:</span><span class=\"at\"> Build and Deploy</span></span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">on</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">  </span><span class=\"fu\">push</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">    </span><span class=\"fu\">branches</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">      </span><span class=\"kw\">-</span><span class=\"at\"> main</span></span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">jobs</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">  </span><span class=\"fu\">build-and-deploy</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">    </span><span class=\"fu\">runs-on</span><span class=\"kw\">:</span><span class=\"at\"> ubuntu-latest</span></span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">    </span><span class=\"fu\">steps</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-11\"><a href=\"#cb1-11\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">      </span><span class=\"kw\">-</span><span class=\"at\"> </span><span class=\"fu\">name</span><span class=\"kw\">:</span><span class=\"at\"> Checkout üõéÔ∏è</span></span>\n<span id=\"cb1-12\"><a href=\"#cb1-12\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">        </span><span class=\"fu\">uses</span><span class=\"kw\">:</span><span class=\"at\"> actions/checkout@v2.3.1</span></span>\n<span id=\"cb1-13\"><a href=\"#cb1-13\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-14\"><a href=\"#cb1-14\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">      </span><span class=\"kw\">-</span><span class=\"at\"> </span><span class=\"fu\">name</span><span class=\"kw\">:</span><span class=\"at\"> Install and Build üîß</span><span class=\"co\"> # This example project is built using </span></span>\n<span id=\"cb1-15\"><a href=\"#cb1-15\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"co\">      # npm and outputs the result to the &#39;build&#39; folder. Replace with </span></span>\n<span id=\"cb1-16\"><a href=\"#cb1-16\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"co\">      # the commands required to build your project, or remove this step </span></span>\n<span id=\"cb1-17\"><a href=\"#cb1-17\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"co\">      # entirely if your site is pre-built.</span></span>\n<span id=\"cb1-18\"><a href=\"#cb1-18\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">        run</span><span class=\"kw\">: </span><span class=\"ch\">|</span></span>\n<span id=\"cb1-19\"><a href=\"#cb1-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>          npm install</span>\n<span id=\"cb1-20\"><a href=\"#cb1-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>          npm run lint</span>\n<span id=\"cb1-21\"><a href=\"#cb1-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>          npm run build</span>\n<span id=\"cb1-22\"><a href=\"#cb1-22\" aria-hidden=\"true\" tabindex=\"-1\"></a></span>\n<span id=\"cb1-23\"><a href=\"#cb1-23\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">      </span><span class=\"kw\">-</span><span class=\"at\"> </span><span class=\"fu\">name</span><span class=\"kw\">:</span><span class=\"at\"> Deploy üöÄ</span></span>\n<span id=\"cb1-24\"><a href=\"#cb1-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">        </span><span class=\"fu\">uses</span><span class=\"kw\">:</span><span class=\"at\"> JamesIves/github-pages-deploy-action@4.1.4</span></span>\n<span id=\"cb1-25\"><a href=\"#cb1-25\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">        </span><span class=\"fu\">with</span><span class=\"kw\">:</span></span>\n<span id=\"cb1-26\"><a href=\"#cb1-26\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">          </span><span class=\"fu\">branch</span><span class=\"kw\">:</span><span class=\"at\"> gh-pages</span><span class=\"co\"> # The branch the action should deploy to.</span></span>\n<span id=\"cb1-27\"><a href=\"#cb1-27\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"at\">          </span><span class=\"fu\">folder</span><span class=\"kw\">:</span><span class=\"at\"> dist</span><span class=\"co\"> # The folder the action should deploy.</span></span></code></pre></div>\n<h2 id=\"add-any-environment-variables\">Add any environment variables</h2>\n<p><strong>Note:</strong> There is almost certainly a better way to do this. Should probably look into it at some point.</p>\n<ul>\n<li><ol type=\"1\">\n<li>Add any key-value pairs as repository secrets in your repo‚Äôs <b>Settings -&gt; Secrets</b> page. I guess this only makes them available in the pipeline, but not available from the <code>process</code>, which is why you have to do the next step.</li>\n</ol></li>\n<li><ol start=\"2\" type=\"1\">\n<li>Add a <code>Create .env File</code> step before the <code>Install and Build</code> step</li>\n</ol></li>\n</ul>\n<div class=\"sourceCode\" id=\"cb2\"><pre class=\"sourceCode yml\"><code class=\"sourceCode yaml\"><span id=\"cb2-1\"><a href=\"#cb2-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"co\"># .github/workflows/gh_pages_deploy.yml snippet</span></span>\n<span id=\"cb2-2\"><a href=\"#cb2-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">-</span><span class=\"at\"> </span><span class=\"fu\">name</span><span class=\"kw\">:</span><span class=\"at\"> Create .env File</span></span>\n<span id=\"cb2-3\"><a href=\"#cb2-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">  run</span><span class=\"kw\">: </span><span class=\"ch\">|</span></span>\n<span id=\"cb2-4\"><a href=\"#cb2-4\" aria-hidden=\"true\" tabindex=\"-1\"></a>    touch .env</span>\n<span id=\"cb2-5\"><a href=\"#cb2-5\" aria-hidden=\"true\" tabindex=\"-1\"></a>    echo API_KEY=${{ secrets.API_KEY }} &gt;&gt; .env</span>\n<span id=\"cb2-6\"><a href=\"#cb2-6\" aria-hidden=\"true\" tabindex=\"-1\"></a>    echo AUTH_DOMAIN=${{ secrets.AUTH_DOMAIN }} &gt;&gt; .env</span></code></pre></div>\n<h2 id=\"references\">References</h2>\n<ul>\n<li>https://github.blog/2020-09-25-github-action-hero-james-ives-and-github-pages-deploy/</li>\n<li>https://github.com/JamesIves/github-pages-deploy-action</li>\n<li>https://github.com/marketplace/actions/deploy-to-github-pages</li>\n</ul>\n"
  },
  {
    "_id": "init_npm_project",
    "_body": "<h2 id=\"initialize-project\">Initialize Project</h2>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">mkdir</span> my_project</span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"bu\">cd</span> my_project</span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">git</span> init</span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"bu\">echo</span> <span class=\"st\">&quot;node_modules&quot;</span> <span class=\"op\">&gt;&gt;</span> .gitignore</span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"bu\">echo</span> <span class=\"st\">&quot;build&quot;</span> <span class=\"op\">&gt;&gt;</span> .gitignore</span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> init <span class=\"at\">-y</span></span></code></pre></div>\n<h2 id=\"update-package.json\">Update <code>package.json</code></h2>\n<p>Set your package.json to look similar to the example below.</p>\n<p>Note the <code>main</code> and <code>types</code> attributes. Other things (like the npm <code>scripts</code>) depend on them</p>\n<div class=\"sourceCode\" id=\"cb2\"><pre class=\"sourceCode json\"><code class=\"sourceCode json\"><span id=\"cb2-1\"><a href=\"#cb2-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">package.json</span></span>\n<span id=\"cb2-2\"><a href=\"#cb2-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">{</span></span>\n<span id=\"cb2-3\"><a href=\"#cb2-3\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;name&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;my_project&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-4\"><a href=\"#cb2-4\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;version&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;0.0.1&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-5\"><a href=\"#cb2-5\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;description&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;my_project&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-6\"><a href=\"#cb2-6\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;main&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;build/index.js&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-7\"><a href=\"#cb2-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;types&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;build/index.d.ts&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-8\"><a href=\"#cb2-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;scripts&quot;</span><span class=\"fu\">:</span> <span class=\"fu\">{</span></span>\n<span id=\"cb2-9\"><a href=\"#cb2-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;build&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;rimraf ./build &amp;&amp; tsc&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-10\"><a href=\"#cb2-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;start&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;npm run build &amp;&amp; node build/index.js&quot;</span></span>\n<span id=\"cb2-11\"><a href=\"#cb2-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"fu\">},</span></span>\n<span id=\"cb2-12\"><a href=\"#cb2-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;repository&quot;</span><span class=\"fu\">:</span> <span class=\"fu\">{</span></span>\n<span id=\"cb2-13\"><a href=\"#cb2-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;type&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;git&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-14\"><a href=\"#cb2-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;url&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;git+https://github.com/brombaut/my_project.git&quot;</span></span>\n<span id=\"cb2-15\"><a href=\"#cb2-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"fu\">},</span></span>\n<span id=\"cb2-16\"><a href=\"#cb2-16\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;keywords&quot;</span><span class=\"fu\">:</span> <span class=\"ot\">[]</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-17\"><a href=\"#cb2-17\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;author&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;Ben Rombaut&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-18\"><a href=\"#cb2-18\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;license&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;ISC&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-19\"><a href=\"#cb2-19\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;private&quot;</span><span class=\"fu\">:</span> <span class=\"kw\">false</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-20\"><a href=\"#cb2-20\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;bugs&quot;</span><span class=\"fu\">:</span> <span class=\"fu\">{</span></span>\n<span id=\"cb2-21\"><a href=\"#cb2-21\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;url&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;https://github.com/brombaut/my_project/issues&quot;</span></span>\n<span id=\"cb2-22\"><a href=\"#cb2-22\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"fu\">},</span></span>\n<span id=\"cb2-23\"><a href=\"#cb2-23\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;homepage&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;https://github.com/brombaut/my_project#readme&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-24\"><a href=\"#cb2-24\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">}</span></span></code></pre></div>\n<h2 id=\"install-basic-dependencies\">Install basic dependencies</h2>\n<p>TypeScript has Implicit, Explicit, and Ambient types. Ambient types are types that get added to the global execution scope. Since we‚Äôre using Node, it would be good if we could get type safety and auto-completion on the Node apis like <code>file</code>, <code>path</code>, <code>process</code>, etc. That‚Äôs what installing the DefinitelyTyped type definition for Node will do.</p>\n<div class=\"sourceCode\" id=\"cb3\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb3-1\"><a href=\"#cb3-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> install typescript <span class=\"at\">--save-dev</span></span>\n<span id=\"cb3-2\"><a href=\"#cb3-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> install @types/node <span class=\"at\">--save-dev</span></span>\n<span id=\"cb3-3\"><a href=\"#cb3-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> install rimraf</span></code></pre></div>\n<h2 id=\"create-tsconfig.json\">Create <code>tsconfig.json</code></h2>\n<div class=\"sourceCode\" id=\"cb4\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb4-1\"><a href=\"#cb4-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">tsc</span> <span class=\"at\">--init</span></span></code></pre></div>\n<ul>\n<li><strong>target</strong>: We want to compile to es5 since we want to build a package with browser compatibility.</li>\n<li><strong>module</strong>: Use commonjs for compatibility.</li>\n<li><strong>declaration</strong>: When you building packages, this should be true. Typescript will then also export type definitions together with the compiled javascript code so the package can be used with both Typescript and Javascript.</li>\n<li><strong>outDir</strong>: The javascript will be compiled to the lib folder.</li>\n<li><strong>include</strong>: All source files in the src folder</li>\n<li><strong>exclude</strong>: We don‚Äôt want to transpile node_modules, neither tests since these are only used during development.</li>\n</ul>\n<div class=\"sourceCode\" id=\"cb5\"><pre class=\"sourceCode json\"><code class=\"sourceCode json\"><span id=\"cb5-1\"><a href=\"#cb5-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">minimum</span> <span class=\"er\">tsconfig.json</span></span>\n<span id=\"cb5-2\"><a href=\"#cb5-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">if</span> <span class=\"er\">you</span> <span class=\"er\">use</span> <span class=\"er\">tsc</span> <span class=\"er\">--init,</span> <span class=\"er\">it</span> <span class=\"er\">will</span> <span class=\"er\">look</span> <span class=\"er\">different,</span></span>\n<span id=\"cb5-3\"><a href=\"#cb5-3\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">but</span> <span class=\"er\">you</span> <span class=\"er\">should</span> <span class=\"er\">make</span> <span class=\"er\">sure</span> <span class=\"er\">to</span> <span class=\"er\">copy</span> <span class=\"er\">the</span> <span class=\"er\">&quot;outDir&quot;,</span></span>\n<span id=\"cb5-4\"><a href=\"#cb5-4\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">&quot;include&quot;,</span> <span class=\"er\">and</span> <span class=\"er\">&quot;exclude&quot;</span> <span class=\"er\">attributes.</span></span>\n<span id=\"cb5-5\"><a href=\"#cb5-5\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">{</span></span>\n<span id=\"cb5-6\"><a href=\"#cb5-6\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;compilerOptions&quot;</span><span class=\"fu\">:</span> <span class=\"fu\">{</span></span>\n<span id=\"cb5-7\"><a href=\"#cb5-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;target&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;es5&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb5-8\"><a href=\"#cb5-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;module&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;commonjs&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb5-9\"><a href=\"#cb5-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;declaration&quot;</span><span class=\"fu\">:</span> <span class=\"kw\">true</span><span class=\"fu\">,</span></span>\n<span id=\"cb5-10\"><a href=\"#cb5-10\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;outDir&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;./build&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb5-11\"><a href=\"#cb5-11\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;rootDir&quot;</span><span class=\"fu\">:</span> <span class=\"st\">&quot;./src&quot;</span><span class=\"fu\">,</span> </span>\n<span id=\"cb5-12\"><a href=\"#cb5-12\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;strict&quot;</span><span class=\"fu\">:</span> <span class=\"kw\">true</span></span>\n<span id=\"cb5-13\"><a href=\"#cb5-13\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"fu\">},</span></span>\n<span id=\"cb5-14\"><a href=\"#cb5-14\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;include&quot;</span><span class=\"fu\">:</span> <span class=\"ot\">[</span><span class=\"st\">&quot;src&quot;</span><span class=\"ot\">]</span><span class=\"fu\">,</span></span>\n<span id=\"cb5-15\"><a href=\"#cb5-15\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;exclude&quot;</span><span class=\"fu\">:</span> <span class=\"ot\">[</span><span class=\"st\">&quot;node_modules&quot;</span><span class=\"ot\">,</span> <span class=\"st\">&quot;**/__tests__/*&quot;</span><span class=\"ot\">]</span></span>\n<span id=\"cb5-16\"><a href=\"#cb5-16\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">}</span></span></code></pre></div>\n"
  },
  {
    "_id": "publish_npm_project",
    "_body": "<h1>\nPublish a TypeScript project to NPM\n</h1>\n<h2 id=\"add-publishing-related-scripts\">Add publishing related scripts</h2>\n<p>Only take what you need form this (e.g., if you don‚Äôt have linting, you don‚Äôt need <code>preversion</code>). At a minimum, you should have <code>prepare</code>, <code>version</code>, and <code>postversion</code>.</p>\n<ul>\n<li><strong>prepare</strong> will run both BEFORE the package is packed and published, and on local npm install. Perfect for running building the code. Add this script to package.json</li>\n<li><strong>prepublishOnly</strong> will run BEFORE prepare and ONLY on npm publish. Here we will run our test and lint to make sure we don‚Äôt publish bad code</li>\n<li><strong>preversion</strong> will run before bumping a new package version. To be extra sure that we‚Äôre not bumping a version with bad code, why not run lint here as well?</li>\n<li><strong>version</strong> will run after a new version has been bumped. If your package has a git repository, like in our case, a commit and a new version-tag will be made every time you bump a new version. This command will run BEFORE the commit is made. One idea is to run the formatter here and so no ugly code will pass into the new version:</li>\n<li><strong>postversion</strong> will run after the commit has been made. A perfect place for pushing the commit as well as the tag.</li>\n</ul>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode json\"><code class=\"sourceCode json\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">...package.json</span> <span class=\"er\">snippet</span></span>\n<span id=\"cb1-2\"><a href=\"#cb1-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">{</span></span>\n<span id=\"cb1-3\"><a href=\"#cb1-3\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;scripts&quot;</span><span class=\"fu\">:</span> <span class=\"fu\">{</span></span>\n<span id=\"cb1-4\"><a href=\"#cb1-4\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;prepare&quot;</span> <span class=\"fu\">:</span> <span class=\"st\">&quot;npm run build&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb1-5\"><a href=\"#cb1-5\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;prepublishOnly&quot;</span> <span class=\"fu\">:</span> <span class=\"st\">&quot;npm test &amp;&amp; npm run lint&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb1-6\"><a href=\"#cb1-6\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;preversion&quot;</span> <span class=\"fu\">:</span> <span class=\"st\">&quot;npm run lint&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb1-7\"><a href=\"#cb1-7\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;version&quot;</span> <span class=\"fu\">:</span> <span class=\"st\">&quot;npm run format &amp;&amp; git add -A src&quot;</span><span class=\"fu\">,</span></span>\n<span id=\"cb1-8\"><a href=\"#cb1-8\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"dt\">&quot;postversion&quot;</span> <span class=\"fu\">:</span> <span class=\"st\">&quot;git push &amp;&amp; git push --tags&quot;</span></span>\n<span id=\"cb1-9\"><a href=\"#cb1-9\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"fu\">}</span></span>\n<span id=\"cb1-10\"><a href=\"#cb1-10\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">}</span></span></code></pre></div>\n<h2 id=\"only-include-what-you-need-in-your-npm-package\">Only include what you need in your npm package</h2>\n<p>Add the files attribute to package.json. This assumes your output build folder is <code>lib</code>.</p>\n<div class=\"sourceCode\" id=\"cb2\"><pre class=\"sourceCode json\"><code class=\"sourceCode json\"><span id=\"cb2-1\"><a href=\"#cb2-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"er\">//</span> <span class=\"er\">...package.json</span> <span class=\"er\">snippet</span></span>\n<span id=\"cb2-2\"><a href=\"#cb2-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">{</span></span>\n<span id=\"cb2-3\"><a href=\"#cb2-3\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"dt\">&quot;files&quot;</span><span class=\"fu\">:</span> <span class=\"ot\">[</span></span>\n<span id=\"cb2-4\"><a href=\"#cb2-4\" aria-hidden=\"true\" tabindex=\"-1\"></a>    <span class=\"st\">&quot;lib/**/*&quot;</span></span>\n<span id=\"cb2-5\"><a href=\"#cb2-5\" aria-hidden=\"true\" tabindex=\"-1\"></a>  <span class=\"ot\">]</span><span class=\"fu\">,</span></span>\n<span id=\"cb2-6\"><a href=\"#cb2-6\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">}</span></span></code></pre></div>\n<h2 id=\"commit-and-push-to-git\">Commit and push to git</h2>\n<div class=\"sourceCode\" id=\"cb3\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb3-1\"><a href=\"#cb3-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">git</span> add <span class=\"at\">-A</span> <span class=\"kw\">&amp;&amp;</span> <span class=\"fu\">git</span> commit <span class=\"at\">-m</span> <span class=\"st\">&quot;Setup Package&quot;</span></span>\n<span id=\"cb3-2\"><a href=\"#cb3-2\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"fu\">git</span> push</span></code></pre></div>\n<h2 id=\"publish-to-npm\">Publish to NPM</h2>\n<p>First login in console</p>\n<div class=\"sourceCode\" id=\"cb4\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb4-1\"><a href=\"#cb4-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> login</span></code></pre></div>\n<p>Then publish. If you are using scoped packages, you have to add the <code>--access public</code> flag.</p>\n<div class=\"sourceCode\" id=\"cb5\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb5-1\"><a href=\"#cb5-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> publish <span class=\"at\">--access</span> public</span></code></pre></div>\n<h2 id=\"bump-version\">Bump version</h2>\n<div class=\"sourceCode\" id=\"cb6\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb6-1\"><a href=\"#cb6-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> version patch</span></code></pre></div>\n<p>Our preversion, version, and postversion will run, create a new tag in git and push it to our remote repository. Now publish again.</p>\n<div class=\"sourceCode\" id=\"cb7\"><pre class=\"sourceCode bash\"><code class=\"sourceCode bash\"><span id=\"cb7-1\"><a href=\"#cb7-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"ex\">npm</span> publish <span class=\"at\">--access</span> public</span></code></pre></div>\n"
  },
  {
    "_id": "practical_statistics",
    "_body": "<h1 id=\"practical-analysis-for-data-scientists\">Practical Analysis for Data Scientists</h1>\n<p>Key term and ideas from the book.</p>\n<h1 id=\"chapter-1---exploratory-data-analysis\">Chapter 1 - Exploratory Data Analysis</h1>\n<h2 id=\"data-types\">Data Types</h2>\n<h3 id=\"key-terms\">Key Terms</h3>\n<ul>\n<li><p><strong>Numeric</strong></p>\n<p>Data that is expressed on a numeric scale</p>\n<ul>\n<li><p><strong>Continuous</strong></p>\n<p>Data that can take on any value in an interval (Synonyms: interval, float, numeric)</p></li>\n<li><p><strong>Discrete</strong></p>\n<p>Data that can take on only integer values, such as counts. (Synonyms: integer, count)</p></li>\n</ul></li>\n<li><p><strong>Categorical</strong></p>\n<p>Data that can take on only a specific set of values representing a set of possible categories. (Synonyms: enum, enumerated, factors, nominal)</p>\n<ul>\n<li><p><strong>Binary</strong></p>\n<p>A special case of categorical data with just two categories of values, e.g., 0/1, true/false. (Synonyms: dichotomous, logical, indicator, boolean)</p></li>\n<li><p><strong>Ordinal</strong></p>\n<p>Categorical data that has an explicit ordering. (Synonym: ordered factor)</p></li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas\">Key Ideas</h3>\n<ul>\n<li>Data is typically classified in software by type.</li>\n<li>Data types include numeric (continuous, discrete) and categorical (binary, ordinal).</li>\n<li>Data typing in software acts as a signal to the software on how to process the data.</li>\n</ul>\n<h2 id=\"rectangular-data\">Rectangular Data</h2>\n<h3 id=\"key-terms-1\">Key Terms</h3>\n<ul>\n<li><p><strong>Data frame</strong></p>\n<p>Rectangular data (like a spreadsheet) is the basic data structure for statistical and machine learning models.</p></li>\n<li><p><strong>Feature</strong></p>\n<p>A column within a table is commonly regerred to as a feature. (Synonyms: attribute, input, predictor, variable)</p></li>\n<li><p><strong>Outcome</strong></p>\n<p>Many data science projects involve predicting an outcome - often a yes/no outcome. The <em>features</em> are sometimes used to predict the <em>outcome</em> in an experiment or a study. (Synonyms: dependent variable, response, target, output)</p></li>\n<li><p><strong>Records</strong></p>\n<p>A row within a table is commonly referred to as a record. (Synonyms: case, example, instance obersvation pattern, sample)</p></li>\n</ul>\n<h3 id=\"key-ideas-1\">Key Ideas</h3>\n<ul>\n<li>The basic data structure in data science is a rectangular matrix in which rows are records and columns are variables (features).</li>\n<li>Terminology can be confusing: there are a variety of synonyms arising from the different disciplines that contribute to data science (statistics, computer science, and IT).</li>\n</ul>\n<h2 id=\"estimates-of-location\">Estimates of Location</h2>\n<h3 id=\"key-terms-2\">Key Terms</h3>\n<ul>\n<li><p><strong>Mean</strong></p>\n<p>The sum of all values divided by the number of values. (Synonym: average)</p></li>\n<li><p><strong>Weighted Mean</strong></p>\n<p>The sum of all values times a weight divided by the sum of the weights. (Synonym: weighted average)</p></li>\n<li><p><strong>Median</strong></p>\n<p>The value sucht hat one-half of the data lies above and below. (Synonym: 50th percentile)</p></li>\n<li><p><strong>Percentile</strong></p>\n<p>The value such that P percent of the data lies below. (Synonym: quantile)</p></li>\n<li><p><strong>Weighted Median</strong></p>\n<p>The value such that one-half of the sum of the weights lies aboe and below the sorted data.</p></li>\n<li><p><strong>Trimmed Mean</strong></p>\n<p>The average of all values after dropping a fixed number of extreme values. (Synonym: truncated mean)</p></li>\n<li><p><strong>Robust</strong></p>\n<p>Not sensitive to extreme values. (Synonym: resistant)</p></li>\n<li><p><strong>Outlier</strong></p>\n<p>A data value that is very different from most of the data. (Synonym: extreme value)</p></li>\n</ul>\n<h3 id=\"key-ideas-2\">Key Ideas</h3>\n<ul>\n<li>The basic metric for location is the mean, but it can be sensitive to extreme values (outliers).</li>\n<li>Other metrics (median, trimmed mean) are less sensitive to outliers and unusual distributions and hence are more robust.</li>\n</ul>\n<h2 id=\"variability-metrics\">Variability Metrics</h2>\n<h3 id=\"key-terms-3\">Key Terms</h3>\n<ul>\n<li><p><strong>Deviations</strong></p>\n<p>The difference between the observed values and the estimate of location. (Synonyms: errors, residuals)</p></li>\n<li><p><strong>Variance</strong></p>\n<p>The sum of squared deviations from the mean divided by <em>n-1</em> where <em>n</em> is the number of data values. (Synonym: mean-squared error)</p></li>\n<li><p><strong>Standard Deviation</strong></p>\n<p>The square root of the variance.</p></li>\n<li><p><strong>Mean Absolute Deviation</strong></p>\n<p>The mean of the absolute values of the deviations from the mean. (Synonyms: l1-norm, Manhattan norm)</p></li>\n<li><p><strong>Median Absolute deviation from the median</strong></p>\n<p>The median of the absolute values of the deviations from the median.</p></li>\n<li><p><strong>Range</strong></p>\n<p>The difference between the largest and the smallest values in the data set.</p></li>\n<li><p><strong>Order Statistics</strong></p>\n<p>Metrics based on the data values sorted from smallest to biggest. (Synonym: rank)</p></li>\n<li><p><strong>Percentile</strong></p>\n<p>The value such that P percent of the values take on this value or less and (100-P) percent take on this value or more. (Synonym: quantile)</p></li>\n<li><p><strong>Interquartile Range</strong></p>\n<p>The difference between the 75th percentile and the 25th percentile. (Synonym: IQR)</p></li>\n</ul>\n<h3 id=\"key-ideas-3\">Key Ideas</h3>\n<ul>\n<li>Variance and standard deviation are the most widespread and routinely reported statistics of variability</li>\n<li>Both are sensitive to outliers.</li>\n<li>More robust metrics include mean absolute deviation, median absolute deviation from the median, and percentiles (quantiles)</li>\n</ul>\n<h2 id=\"exploring-the-distribution\">Exploring the Distribution</h2>\n<h3 id=\"key-terms-4\">Key Terms</h3>\n<ul>\n<li><p><strong>Boxplot</strong></p>\n<p>A plot introduced by Turkey as a quick way to visualize the distribution of data. (Synonym: box and whiskers plot)</p></li>\n<li><p><strong>Frequency Table</strong></p>\n<p>A tally of the count of numeric data values that fall into a set of intervals (bins)</p></li>\n<li><p><strong>Histogram</strong></p>\n<p>A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis. While visually similar, bar charts should not be confused with histograms.</p></li>\n<li><p><strong>Density plot</strong></p>\n<p>A smoothed version of the histogram, often based on a <em>kernel density estimate</em>.</p></li>\n</ul>\n<h3 id=\"key-ideas-4\">Key Ideas</h3>\n<ul>\n<li>A freqency histogram plots frequency counts on the y-axis and variable values on the x-axis: it gives a sense of the distribution of the data at a glance</li>\n<li>A frequency table is a tabular version of the frequency counts found in a histogram.</li>\n<li>A boxplot - with the top and bottom of the box at the 75th and 25th percentiles, respectively - also gives a quick sens of the distribution of the data; it is often used in side-by-side displays to compare distributions.</li>\n<li>A density plot is a smoothed version of a histogram; it requires a function to estimate a plot based on the data (multiple estimates are possible, of course)</li>\n</ul>\n<h2 id=\"exploring-categorical-data\">Exploring Categorical Data</h2>\n<h3 id=\"key-terms-5\">Key Terms</h3>\n<ul>\n<li><p><strong>Mode</strong></p>\n<p>The most commonly occuring category or value in a data set.</p></li>\n<li><p><strong>Expected value</strong></p>\n<p>When the categories can be associated with a numeric value, this gives an average value based on a category‚Äôs probability of occurence.</p></li>\n<li><p><strong>Bar Charts</strong></p>\n<p>The frequency or proportion for each category plotted as bars.</p></li>\n<li><p><strong>Pie Charts</strong></p>\n<p>The frequency or proportion for each category plotted as wedges in a pie.</p></li>\n</ul>\n<h3 id=\"key-ideas-5\">Key Ideas</h3>\n<ul>\n<li>Categorical data is typically summed up in proportions and can be visualized in a bar chart.</li>\n<li>Categories might represent distinct things (apples and oranges, male and female), levels of a factor variable (low, medium, high), or numeric data that has been binned.</li>\n<li>Expected value is the sum of values times their probability of occurrence, often used to sum up factor variable lengths.</li>\n</ul>\n<h2 id=\"correlation\">Correlation</h2>\n<h3 id=\"key-terms-6\">Key Terms</h3>\n<ul>\n<li><p><strong>Correlation Coefficient</strong></p>\n<p>A metric that measures the extent to which numeric variables are associated with one another (ranges from -1 to +1).</p></li>\n<li><p><strong>Correlation Matrix</strong></p>\n<p>A table where the variables are shown on both rows and columns, and the cell values are the correlations between variables.</p></li>\n<li><p><strong>Scatterplot</strong></p>\n<p>A plot in which the x-axis is the value of one variable, and the y-axis the value of another.</p></li>\n</ul>\n<h3 id=\"key-ideas-6\">Key Ideas</h3>\n<ul>\n<li>The correlation coefficient measures the extent to which two paired variables (e.g.¬†height and weight for individuals) are associated with one another.</li>\n<li>When high values of v1 go with high values of v2, v1 and v2 are positively associated.</li>\n<li>When high values of v1 go with low values of v2, v1 and v2 are negatively associated.</li>\n<li>The correlation coefficient is a standardized metric, so that it always ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation).</li>\n<li>A correlation coefficient of zero indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the correlation coefficient just by chance.</li>\n</ul>\n<h2 id=\"exploring-two-or-more-variables\">Exploring Two or more Variables</h2>\n<h3 id=\"key-terms-7\">Key Terms</h3>\n<ul>\n<li><p><strong>Contingency Table</strong></p>\n<p>A tally of counts between two eor more categorical variables.</p></li>\n<li><p><strong>Hexagonal Binning</strong></p>\n<p>A plot of two numeric variables with the records binned into hexagons.</p></li>\n<li><p><strong>Contour Plot</strong></p>\n<p>A plot showing the density of two numeric variables like a topographical map.</p></li>\n<li><p><strong>Violin Plot</strong></p>\n<p>Similar to a boxplot but showing the density estimate.</p></li>\n</ul>\n<h3 id=\"key-ideas-7\">Key Ideas</h3>\n<ul>\n<li>Hexagonal binning and contour plots are useful tools that permit graphical examination of two numeric variables at a time, without being overwhelmed by huge amounts of data.</li>\n<li>Contingency tables are the standard tool for looking at the counts of two categorical variables.</li>\n<li>Boxplots and violin plots allow you to plot a numeric variable against a categorical variable.</li>\n</ul>\n<h1 id=\"chapter-2---data-and-sampling-distributions\">Chapter 2 - Data and Sampling Distributions</h1>\n<h2 id=\"random-sampling\">Random Sampling</h2>\n<h3 id=\"key-terms-8\">Key Terms</h3>\n<ul>\n<li><p><strong>Sample</strong></p>\n<ul>\n<li>A subset from a larger data set.</li>\n</ul></li>\n<li><p><strong>Population</strong></p>\n<ul>\n<li>The larger data set or idea of a data set.</li>\n</ul></li>\n<li><p><strong>N (n)</strong></p>\n<ul>\n<li>The size of the population (sample).</li>\n</ul></li>\n<li><p><strong>Random Sampling</strong></p>\n<ul>\n<li>Drawing elements into a sample at random.</li>\n</ul></li>\n<li><p><strong>Stratified Sampling</strong></p>\n<ul>\n<li>Dividing the population into strata and randomly sampling from each strata.</li>\n</ul></li>\n<li><p><strong>Stratum (pl., strata)</strong></p>\n<ul>\n<li>A homogeneous subgroup of a population with common characteristics.</li>\n</ul></li>\n<li><p><strong>Simple Random Sample</strong></p>\n<ul>\n<li>The sample that results from random sampling without stratifying the population.</li>\n</ul></li>\n<li><p><strong>Bias</strong></p>\n<ul>\n<li>Systematic error.</li>\n</ul></li>\n<li><p><strong>Sample Bias</strong></p>\n<ul>\n<li>A sample that misrepresents the population.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-8\">Key Ideas</h3>\n<ul>\n<li>Even in the era of big data, random sampling remains an important arrow in the data scientist‚Äôs quiver.</li>\n<li>Bias occurs when measurements or observations are systematically in error because they are not representative of the full population.</li>\n<li>Data quality is often more important than data quantity, and random sampling can reduce bias and facilitate quality improvements that would otherwise be prohibitively expensive.</li>\n</ul>\n<h2 id=\"selection-bias\">Selection Bias</h2>\n<h3 id=\"key-terms-9\">Key Terms</h3>\n<ul>\n<li><p><strong>Selection Bias</strong></p>\n<ul>\n<li>Bias resulting from the way in which observations are selected.</li>\n</ul></li>\n<li><p><strong>Data Snooping</strong></p>\n<ul>\n<li>Extensive hunting through data in search of something interesting.</li>\n</ul></li>\n<li><p><strong>Vast search effect</strong></p>\n<ul>\n<li>Bias or nonreproducibility resulting from repeated data modeling, or modeling data with large numbers of predictor variables.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-9\">Key Ideas</h3>\n<ul>\n<li>Specifying a hypothesis and then collecting data following randomization and random sampling principles ensures against bias.</li>\n<li>All other forms of data analysis run the risk of bias resulting from the data collection/analysis process (repeated running of models in data mining, data snooping in research, and after-the-fact selection of interesting events).</li>\n</ul>\n<h2 id=\"sampling-distribution\">Sampling Distribution</h2>\n<h3 id=\"key-terms-10\">Key Terms</h3>\n<ul>\n<li><p><strong>Sample Statistic</strong></p>\n<ul>\n<li>A metric calculated for a sample of data drawn from a larger population.</li>\n</ul></li>\n<li><p><strong>Data Distribution</strong></p>\n<ul>\n<li>The frequency distribution of individual values in a data set.</li>\n</ul></li>\n<li><p><strong>Sampling Distribution</strong></p>\n<ul>\n<li>The frequency distribution of a sample statistic over many samples or resamples.</li>\n</ul></li>\n<li><p><strong>Central Limit Theorem</strong></p>\n<ul>\n<li>The tendency of the sampling distribution to take on a normal shape as a sample size rises.</li>\n</ul></li>\n<li><p><strong>Standard Error</strong></p>\n<ul>\n<li>The variability (standard deviation) of a sample <em>statistic</em> over many values (not to be confused wuth <em>standard deviation</em>, which by itself, refers to variability of individual data <em>values</em>).</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-10\">Key Ideas</h3>\n<ul>\n<li>The frequency distribution of a sample statistic tells us how that metric would turn out differently from sample to sample.</li>\n<li>This sampling distribution can be estimated via the bootstrap, or via formulas that rely on the central limit theorem.</li>\n<li>A key metric that sums up the variability of a sample statistic is its standard error.</li>\n</ul>\n<h2 id=\"the-bootstrap\">The Bootstrap</h2>\n<h3 id=\"key-terms-11\">Key Terms</h3>\n<ul>\n<li><p><strong>Bootstrap Sample</strong></p>\n<ul>\n<li>A sample taken with replacement from an observed data set.</li>\n</ul></li>\n<li><p><strong>Resampling</strong></p>\n<ul>\n<li>The process of taking repeated samples from observed data; includes both bootstrap and permutation (shuffling) procedures.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-11\">Key Ideas</h3>\n<ul>\n<li>The bootstrap (sampling with replacement from a data set) is a powerful tool for assessing the variability of a sample statistic.</li>\n<li>The bootstrap can be applied in similar fashion in a wide variety of circumstances, without extensive study of mathematical approxumations to sampling distributions.</li>\n<li>It also allows us to estimate sampling distributions for statistics where no mathematical approximation has been developed.</li>\n<li>When applied to predictive models, aggregating multiple bootstrap sample predictions (bagging) outperforms the use of a single model.</li>\n</ul>\n<h2 id=\"confidence-intervals\">Confidence Intervals</h2>\n<h3 id=\"key-terms-12\">Key Terms</h3>\n<ul>\n<li><p><strong>Confidence Level</strong></p>\n<ul>\n<li>The percentage of confidence intervals, constructed in the same way from the same population, that are expected to contain the statistic of interest.</li>\n</ul></li>\n<li><p><strong>Interval Endpoints</strong></p>\n<ul>\n<li>The top and bottom of the confidence interval.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-12\">Key Ideas</h3>\n<ul>\n<li>Confidence intervals are the typical way to present estimates as an interval range.</li>\n<li>The more data you have, the less variable a sample estimate will be.</li>\n<li>The lower the level of confidence you can tolerate, the narrower the confidence interval will be.</li>\n<li>The bootstrap is an effective way to construct confidence intervals.</li>\n</ul>\n<h2 id=\"normal-distribution\">Normal Distribution</h2>\n<h3 id=\"key-terms-13\">Key Terms</h3>\n<ul>\n<li><p><strong>Error</strong></p>\n<ul>\n<li>The difference between a data point and a predicted or average value.</li>\n</ul></li>\n<li><p><strong>Standardize</strong></p>\n<ul>\n<li>Subtract the mean and divide by the standard deviation.</li>\n</ul></li>\n<li><p><strong>z-score</strong></p>\n<ul>\n<li>The result of standardizing an individual data point.</li>\n</ul></li>\n<li><p><strong>Standard normal</strong></p>\n<ul>\n<li>A normal distribution with mean = 0 and standard deviation = 1.</li>\n</ul></li>\n<li><p><strong>QQ-Plot</strong></p>\n<ul>\n<li>A plot to visualize how close a sample distribution is to a specified distribution. e.g.¬†normal distribution.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-13\">Key Ideas</h3>\n<ul>\n<li>The normal distribution was essential to the historical development of statistics, as it permitted mathematical approximations of uncertainty and variability.</li>\n<li>While raw data is typically not normally distributed, errors often are, as are averages and totals in large samples.</li>\n<li>To convert data to <em>z-scores</em>, you subtract the mean of the data and divide by the standard deviation; you can then compare the data to a normal distribution.</li>\n</ul>\n<h2 id=\"long-tailed-distribution\">Long-Tailed Distribution</h2>\n<h3 id=\"key-terms-14\">Key Terms</h3>\n<ul>\n<li><p><strong>Tail</strong></p>\n<ul>\n<li>The long narrow portion of a frequency distribution, where relatively extreme values occure at low frequency.</li>\n</ul></li>\n<li><p><strong>Skew</strong></p>\n<ul>\n<li>Where one tail of a distribution is longer than the other.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-14\">Key Ideas</h3>\n<ul>\n<li>Most data is not normally distributed.</li>\n<li>Assuming a normal distribution can lead to underestimates of extreme events (‚Äúblack swans‚Äù).</li>\n</ul>\n<h2 id=\"students-t-distribution\">Students t-Distribution</h2>\n<h3 id=\"key-terms-15\">Key Terms</h3>\n<ul>\n<li><p><strong>n</strong></p>\n<ul>\n<li>Sample size.</li>\n</ul></li>\n<li><p><strong>Degrees of Freedom</strong></p>\n<ul>\n<li>A parameter that allows the t-distribution to adjust to different sample sizes, statistics, and numbers of groups.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-15\">Key Ideas</h3>\n<ul>\n<li>The t-distribution is actually a family of distributions resembling the normal distribution but with thicker tails.</li>\n<li>The t-distribution is widely used as a reference basis for the distribution of sample means, differences between two sample means, regression parameters, and more.</li>\n</ul>\n<h2 id=\"binomial-distribution\">Binomial Distribution</h2>\n<h3 id=\"key-terms-16\">Key Terms</h3>\n<ul>\n<li><p><strong>Trial</strong></p>\n<ul>\n<li>An event with a discrete outcome (e.g.¬†a coin flip).</li>\n</ul></li>\n<li><p><strong>Success</strong></p>\n<ul>\n<li>The outcome of interest for a trial. (Synonym: ‚Äú1‚Äù as opposed to ‚Äú0‚Äù)</li>\n</ul></li>\n<li><p><strong>Binomial</strong></p>\n<ul>\n<li>Having two outcomes.</li>\n</ul></li>\n<li><p><strong>Binomial Trial</strong></p>\n<ul>\n<li>A trial with two outcomes. (Synonym: Bernoulli trial)</li>\n</ul></li>\n<li><p><strong>Binomial Distribution</strong></p>\n<ul>\n<li>Distribution of number of successes in <em>x</em> trials. (Synonym: Bernoulli distribution).</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-16\">Key Ideas</h3>\n<ul>\n<li>Binamial outcomes are important to model, since they represent, among other things, fundamental decisions (buy or don‚Äôt buy, click or don‚Äôt click, survive or die, etc.).</li>\n<li>A binomial trial is an experiment with two possible outcomes: one with probability <em>p</em> and the other with probability <em>1 - p</em>.</li>\n<li>With large <em>n</em>, and provided <em>p</em> is not too close to 0 or 1, the binomial distribution can be approximated by the normal distribution.</li>\n</ul>\n<h2 id=\"chi-square-distribution\">Chi-Square Distribution</h2>\n<h3 id=\"key-ideas-17\">Key Ideas</h3>\n<ul>\n<li>The chi-square distribution is typically concerned with counts of subjects or items falling into categories.</li>\n<li>The chi-square statistic measures the extent of departure from what you would expect in a null model.</li>\n</ul>\n<h2 id=\"f-distribution\">F-Distribution</h2>\n<h3 id=\"key-ideas-18\">Key Ideas</h3>\n<ul>\n<li>The F-distribution is used with experiments and linear models involving measured data.</li>\n<li>The F-statistic compares variation due to factors of interest to overall variation.</li>\n</ul>\n<h2 id=\"poisson-and-related-distributions\">Poisson and Related Distributions</h2>\n<h3 id=\"key-terms-17\">Key Terms</h3>\n<ul>\n<li><p><strong>Lambda</strong></p>\n<ul>\n<li>The rate (per unit of time or space) at which events occur.</li>\n</ul></li>\n<li><p><strong>Poisson Distribution</strong></p>\n<ul>\n<li>The frequency distribution of the number of events in sampled units of time or space.</li>\n</ul></li>\n<li><p><strong>Exponential Distribution</strong></p>\n<ul>\n<li>The frequency distribution of the time or distance from one event to the next event.</li>\n</ul></li>\n<li><p><strong>Weibull Distribution</strong></p>\n<ul>\n<li>A generalized version of the exponential distribution in which the event rate is allowed to shift over time.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-19\">Key Ideas</h3>\n<ul>\n<li>For events that occur at a constant rate, the number of events per unit time or space can be modeled as a Poisson distribution.</li>\n<li>You can also model the time or distance between one event and the next as an exponential distribution.</li>\n<li>A changing event rate over time (e.g., an increasing probability of device failure) can be modeled with the Weibull distribution.</li>\n</ul>\n<h1 id=\"chapter-3---statistical-experiments-and-significance-testing\">Chapter 3 - Statistical Experiments and Significance Testing</h1>\n<h2 id=\"ab-testiing\">A/B Testiing</h2>\n<h3 id=\"key-terms-18\">Key Terms</h3>\n<ul>\n<li><p><strong>Treatment</strong></p>\n<ul>\n<li>Something (drug, price, web headline) to which a subject is exposed.</li>\n</ul></li>\n<li><p><strong>Treatment Group</strong></p>\n<ul>\n<li>A group of subjects exposed to a specific treatment.</li>\n</ul></li>\n<li><p><strong>Control Group</strong></p>\n<ul>\n<li>A group of subjects exposed to no (or standard) treatment.</li>\n</ul></li>\n<li><p><strong>Randomization</strong></p>\n<ul>\n<li>The process of randomly assigning subjects to treatment.</li>\n</ul></li>\n<li><p><strong>Subjects</strong></p>\n<ul>\n<li>The items (web visitors, patients, etc.) that are exposed to treatment.</li>\n</ul></li>\n<li><p><strong>Test Statistics</strong></p>\n<ul>\n<li>The metric used to measure the effect of the treatment.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-20\">Key Ideas</h3>\n<ul>\n<li>Subjects are assigned to two (or more) groups that are treated exactly alike, except that the treatment under study differs from one group to another.</li>\n<li>Ideally, subjects are assigned randomly to the groups.</li>\n</ul>\n<h2 id=\"hypothesis-tests\">Hypothesis Tests</h2>\n<h3 id=\"key-terms-19\">Key Terms</h3>\n<ul>\n<li><p><strong>Null hypothesis</strong></p>\n<ul>\n<li>The hypothesis that chance is to blame.</li>\n</ul></li>\n<li><p><strong>Alternative hypothesis</strong></p>\n<ul>\n<li>Counterpoint to the null (what you hope to prove).</li>\n</ul></li>\n<li><p><strong>One-way test</strong></p>\n<ul>\n<li>Hypothesis test that counts chance results only in one direction.</li>\n</ul></li>\n<li><p><strong>Two-way test</strong></p>\n<ul>\n<li>Hypothesis test that counts chance results in two directions.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-21\">Key Ideas</h3>\n<ul>\n<li>A null hypothesis is a logical contruct embodying the notion that nothing special gas gappened, and any effect you observe is due to random chance.</li>\n<li>The hypothesis test assumes that the null hypothesis is true, creates a ‚Äúnull model‚Äù (a probability model), and tests whether the effect you observe is a reasonable outcome of that model.</li>\n</ul>\n<h2 id=\"resampling\">Resampling</h2>\n<h3 id=\"key-terms-20\">Key Terms</h3>\n<ul>\n<li><p><strong>Permutation Test</strong></p>\n<ul>\n<li>The procedure of combining two or more samples together and randomly (or exhaustively) reallocating the observations to resamples. (Synonyms: Randomization test, random permutation test, exact test).</li>\n</ul></li>\n<li><p><strong>Resampling</strong></p>\n<ul>\n<li>Drawing additionsl samples (‚Äúresamples‚Äù) from an observed data set.</li>\n</ul></li>\n<li><p><strong>With or without replacement</strong></p>\n<ul>\n<li>In sampling, whether or not an item is returned to the sample before the next draw.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-22\">Key Ideas</h3>\n<ul>\n<li>In a permutation test, multiple samples are combined and then shuffled.</li>\n<li>The shuffled values are then divided into resamples, and the statistic of interest is calculated.</li>\n<li>This process is then repeated, and the resampled statistic is tabulated.</li>\n<li>Comparing the observed value of the statistic to the resampled distribution allows you to judge whether an observed difference between samples might occur by chance.</li>\n</ul>\n<h2 id=\"statistical-significant-and-p-values\">Statistical Significant and p-Values</h2>\n<h3 id=\"key-terms-21\">Key Terms</h3>\n<ul>\n<li><p><strong>p-value</strong></p>\n<ul>\n<li>Given a chance model that embodies the null hypothesis, the p-value is the probability of obtaining results as unusual or extreme as the observed results.</li>\n</ul></li>\n<li><p><strong>Alpha</strong></p>\n<ul>\n<li>The probability threshold of ‚Äúunusualness‚Äù that chance results must surpass for actual outcomes to be deemed statistically significant.</li>\n</ul></li>\n<li><p><strong>Type 1 error</strong></p>\n<ul>\n<li>Mistakenly concluding an effect is real (when it is due to chance).</li>\n</ul></li>\n<li><p><strong>Type 2 error</strong></p>\n<ul>\n<li>Mistakenly concluding an effect is due to chance (when it is real).</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-23\">Key Ideas</h3>\n<ul>\n<li>Significant tests are used to determine whether an observed effect is within the range of chance variation for a null hypothesis.</li>\n<li>The p-value is the probability that results as extreme as the observed results might occur, given a null hypothesis model.</li>\n<li>The alpha value is the threshold of ‚Äúunusualness‚Äù in a null hypothesis chance model.</li>\n<li>Significance testing has been much more relevant for formal reporting of research than for data science (but has been fading recently, even for the former).</li>\n</ul>\n<h2 id=\"t-tests\">t-Tests</h2>\n<h3 id=\"key-terms-22\">Key Terms</h3>\n<ul>\n<li><p><strong>Test statistic</strong></p>\n<ul>\n<li>A metric for the difference or effect of interest.</li>\n</ul></li>\n<li><p><strong>t-statistic</strong></p>\n<ul>\n<li>A standardized version of common test statistic such as means.</li>\n</ul></li>\n<li><p><strong>t-distribution</strong></p>\n<ul>\n<li>A reference distribution (in this case derived from the null hypothesis), to which the observed t-statistic can be compared.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-24\">Key Ideas</h3>\n<ul>\n<li>Before the advent of computers, resampling tests were not practical, and statisticians used standard reference distributions.</li>\n<li>A test statistic could then be standardized and compared to the reference distribution.</li>\n<li>One such widely used standardized statistic is the t-statistic.</li>\n</ul>\n<h2 id=\"multiple-testing\">Multiple Testing</h2>\n<h3 id=\"key-terms-23\">Key Terms</h3>\n<ul>\n<li><p><strong>Type 1 error</strong></p>\n<ul>\n<li>Mistakenly concluding that an effect is statistically significant.</li>\n</ul></li>\n<li><p><strong>False recovery rate</strong></p>\n<ul>\n<li>Across multiple tests, the rate of making a Type 1 error.</li>\n</ul></li>\n<li><p><strong>Alpha inflation</strong></p>\n<ul>\n<li>The multiple testing phenomenon, in whic alpha, the probability of making a Type 1 error, increases as you conduct more tests.</li>\n</ul></li>\n<li><p><strong>Adjustment of p-values</strong></p>\n<ul>\n<li>Accounting for doing multiple tests on the same data.</li>\n</ul></li>\n<li><p><strong>Overfitting</strong></p>\n<ul>\n<li>Fitting the noise.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-25\">Key Ideas</h3>\n<ul>\n<li>Multiplicity in a research study or data mining project (multiple comparisons, many variables, many models, etc.) increase the risk of concluding that something is significant just by chance.</li>\n<li>For situations involving multiple statistical comparisons (i.e.¬†multiple tests of significant), there are statistical adjustment procedures.</li>\n<li>In a data mining situation, use of a holdout sample with labeled outcome variables can help avoid misleading results.</li>\n</ul>\n<h2 id=\"degrees-of-freedom\">Degrees of Freedom</h2>\n<h3 id=\"key-terms-24\">Key Terms</h3>\n<ul>\n<li><p><strong>n or sample size</strong></p>\n<ul>\n<li>The number of observations (also called rows or records) in the data.</li>\n</ul></li>\n<li><p><strong>d.f.</strong></p>\n<ul>\n<li>Degrees of freedom.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-26\">Key Ideas</h3>\n<ul>\n<li>The number of degrees of freedom (d.f.) forms part of the calculation to standardize test statistics so they can be compared to reference distributions (t-distributions, F-distributions, etc.).</li>\n<li>The conecpt of degrees of freedom lies behind factoring of categorical variables into n - 1 indicator or dummy variables when doing a regression (to avoid multicollinearity).</li>\n</ul>\n<h2 id=\"anova-analysis-of-variance\">ANOVA (Analysis of Variance)</h2>\n<h3 id=\"key-terms-25\">Key Terms</h3>\n<ul>\n<li><p><strong>Pairwise comparison</strong></p>\n<ul>\n<li>A hypothesis test (e.g.¬†of means) between two groups among multiple groups.</li>\n</ul></li>\n<li><p><strong>Omnibus test</strong></p>\n<ul>\n<li>A single hypothesis test of the overall variance among multiple group means.</li>\n</ul></li>\n<li><p><strong>Decomposition of variance</strong></p>\n<ul>\n<li>Separation of components contributing to an individual value (e.g.¬†from the overall average, from a treatment mean, and from a residual error).</li>\n</ul></li>\n<li><p><strong>F-statistic</strong></p>\n<ul>\n<li>A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.</li>\n</ul></li>\n<li><p><strong>SS</strong></p>\n<ul>\n<li>‚ÄúSum of squares‚Äù, referring to deviations from some average value.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-27\">Key Ideas</h3>\n<ul>\n<li>ANOVA is a statistical procedure for analysing the results of an experiment with multiple groups.</li>\n<li>It is the extension of similar procedures for the A/B test, used to assess whether the overall variation among groups is within the range of chance variation.</li>\n<li>A useful outcome of ANOVA is the identification of variance components associated with group treatments, interaction effects, and errors</li>\n</ul>\n<h2 id=\"chi-square-test\">Chi-Square test</h2>\n<h3 id=\"key-terms-26\">Key Terms</h3>\n<ul>\n<li><p><strong>Chi-square statistic</strong></p>\n<ul>\n<li>A measure of the exten to which some observed data departs from expectation.</li>\n</ul></li>\n<li><p><strong>Expectation or expected</strong></p>\n<ul>\n<li>How we would expect the data to turn out under some assumption, typically the null hypothesis.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-28\">Key Ideas</h3>\n<ul>\n<li>A common procedure in statistics is to test whether observed data counts are consistent with an assumption of independence (e.g., propensity to buy a particular item is independent of gender).</li>\n<li>The chi-square distribution is the reference distribution (which embodies the assumption of independence) to which the observed calculate chi-square statistic must be compared.</li>\n</ul>\n<h2 id=\"multi-arm-bandits\">Multi-Arm Bandits</h2>\n<h3 id=\"key-terms-27\">Key Terms</h3>\n<ul>\n<li><p><strong>Multi-arm bandit</strong></p>\n<ul>\n<li>An imaginary slot machine with multiple arms for the customer to choose from, each with different payoffs, here taken to be an analogy for a multitreatment experiment.</li>\n</ul></li>\n<li><p><strong>Arm</strong></p>\n<ul>\n<li>A treatment in an experiment (e.g., ‚Äúheadline A in a web test‚Äù).</li>\n</ul></li>\n<li><p><strong>Win</strong></p>\n<ul>\n<li>The experimental analog of a win at the slot machine (e.g., ‚Äúcustomer clicks on the link‚Äù).</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-29\">Key Ideas</h3>\n<ul>\n<li>Traditional A/B tests envision a random sampling process, which can lead to excessive exposure to the inferior treatment.</li>\n<li>Multi-arm bandits, in contrast, alter the sampling process to incorporate information learned during the experiment and reduce the frequency of the inferior treatment.</li>\n<li>They also facilitate efficient treatment of more than two treatments.</li>\n<li>There are different algorithms for shifting sampling probability away from the inferior treatment(s) and to the p(presumed) superior one.</li>\n</ul>\n<h2 id=\"power-and-sample-size\">Power and Sample Size</h2>\n<h3 id=\"key-terms-28\">Key Terms</h3>\n<ul>\n<li><p><strong>Effect Size</strong></p>\n<ul>\n<li>The minimum size of the effect that you hope to be able to detect in a statistical test, such as ‚Äú20% improvement in click rates‚Äù.</li>\n</ul></li>\n<li><p><strong>Power</strong></p>\n<ul>\n<li>The probability of detecting a given effect size with a given sample size.</li>\n</ul></li>\n<li><p><strong>Significance level</strong></p>\n<ul>\n<li>The statistical significance level at which the test will be conducted.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-30\">Key Ideas</h3>\n<ul>\n<li>Finding out how big a sample size you need requires thinking ahead to the statistical test you plan to conduct.</li>\n<li>You must specify the minimum size of the effect that you want to detect.</li>\n<li>You must also specify the required probability of detecting that effect size (power).</li>\n<li>Finally, you must specify the significance level (alpha) at which the test will be conducted.</li>\n</ul>\n<h1 id=\"chapter-4---regression-and-prediction\">Chapter 4 - Regression and Prediction</h1>\n<h2 id=\"simple-linear-regression\">Simple Linear Regression</h2>\n<h3 id=\"key-terms-29\">Key Terms</h3>\n<ul>\n<li><p><strong>Response</strong></p>\n<ul>\n<li>The variable we are trying to predict. (Synonys: dependent variable, Y variable, target, outcome)</li>\n</ul></li>\n<li><p><strong>Independent Variable</strong></p>\n<ul>\n<li>The variable used to predict the response. (Synonyms: X variable, feature, attribute, predictor)</li>\n</ul></li>\n<li><p><strong>Record</strong></p>\n<ul>\n<li>The vector of predictor and outcome values for a specific individual or case. (Synonym: row, case, instance, example)</li>\n</ul></li>\n<li><p><strong>Intercept</strong></p>\n<ul>\n<li>The intercept of the regression line - that is, the predicted value when X = 0.</li>\n</ul></li>\n<li><p><strong>Regression coefficient</strong></p>\n<ul>\n<li>The slope of the regression line. (Synonyms: Parameter estimates, weights)</li>\n</ul></li>\n<li><p><strong>Fitted Values</strong></p>\n<ul>\n<li>The estimates obtained from the regression line. (Synonym: predicted values)</li>\n</ul></li>\n<li><p><strong>Residuals</strong></p>\n<ul>\n<li>The difference between the observed values and the fitted values. (Synonym: errors)</li>\n</ul></li>\n<li><p><strong>Least Squares</strong></p>\n<ul>\n<li>The method of fitting a regression by minimizing the sum of squared residuals. (Synonyms: ordinary least squares, OLS)</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-31\">Key Ideas</h3>\n<ul>\n<li>The regression equation models the relationship between a response variable Y and a predictor variable X as a line.</li>\n<li>A regression model yields fitted values and residuals - predictions of the response and the errors of the predictions.</li>\n<li>Regression models are typically fit by the method of least squares.</li>\n<li>Regression is used both for prediction and explanation.</li>\n</ul>\n<h2 id=\"multiple-linear-regression\">Multiple Linear Regression</h2>\n<h3 id=\"key-terms-30\">Key Terms</h3>\n<ul>\n<li><p><strong>Root mean square error</strong></p>\n<ul>\n<li>The square root of the average squared error of the regression (this is the most widely used metric to compare regression models). (Synonym: RMSE)</li>\n</ul></li>\n<li><p><strong>Residual standard error</strong></p>\n<ul>\n<li>The same as the root mean squared error, but adjusted for degrees of freedom. (Synonym: RSE)</li>\n</ul></li>\n<li><p><strong>R-squared</strong></p>\n<ul>\n<li>The proportion of variance explained by the model, from 0 to 1. (Synonym: coefficient of determination)</li>\n</ul></li>\n<li><p><strong>t-statistic</strong></p>\n<ul>\n<li>The coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to compare the importance of variables in the model.</li>\n</ul></li>\n<li><p><strong>Weighted regression</strong></p>\n<ul>\n<li>Regression with the records having different weights.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-32\">Key Ideas</h3>\n<ul>\n<li>Multiple linear regression models the relationship between a response variable Y and multiple predictor variables X1 ‚Ä¶ Xp.</li>\n<li>The most important metrics to evaluate a model are root mean squared error (RMSE) and R-squared (R^2).</li>\n<li>The standard error of the coefficients can be used to measure the reliability of a variable‚Äôs contribution to a model.</li>\n<li>Stepwise regression is a way to automatically determine which variables should be included in the model.</li>\n<li>Weighted regression is used to give certain records more or less weight in fitting the equation.</li>\n</ul>\n<h2 id=\"prediction-using-regression\">Prediction Using Regression</h2>\n<h3 id=\"key-terms-31\">Key Terms</h3>\n<ul>\n<li><p><strong>Prediction Interval</strong></p>\n<ul>\n<li>An uncertainty interval around an individual predicted value.</li>\n</ul></li>\n<li><p><strong>Extrapolation</strong></p>\n<ul>\n<li>Extension of a model beyond the range of the data used to fit it.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-33\">Key Ideas</h3>\n<ul>\n<li>Extrapolation beyond the range of the data can lead to error.</li>\n<li>Confidence intervals quantify uncertainty around regression coefficients.</li>\n<li>Prediction intervals quantify uncertainty in individual predictions.</li>\n<li>Most software, R included, will produce prediction and confidence intervals in default or specified output, using formulas.</li>\n<li>The bootstrap can also be used to produce prediction and confidence intervals; the interpretation and idea are the same.</li>\n</ul>\n<h2 id=\"factor-variables\">Factor Variables</h2>\n<h3 id=\"key-terms-32\">Key Terms</h3>\n<ul>\n<li><p><strong>Dummy Variables</strong></p>\n<ul>\n<li>Binary 0-1 variables derived by recording factor data for use in regression and other models.</li>\n</ul></li>\n<li><p><strong>Reference coding</strong></p>\n<ul>\n<li>The most common type of coding used by statisticians, in which one level of a factor is used as a reference and other factors are compared to that level. (Synonym: treatment coding)</li>\n</ul></li>\n<li><p><strong>One hot encoder</strong></p>\n<ul>\n<li>A common type of coding used in the machine learning community in which all factor levels are retained. While useful for certain machine learning algorithms, this approach is not appropriate for multiple linear regression.</li>\n</ul></li>\n<li><p><strong>Deviation coding</strong></p>\n<ul>\n<li>A type of coding that compares each level against the overall mean as opposed to the reference level. (Synonym: sum contrast)</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-34\">Key Ideas</h3>\n<ul>\n<li>Factor variables need to be converted into numeric variables for us in a regression.</li>\n<li>The most common method to encode a factor variable with P distinct values is to represent them using P - 1 dummy variables.</li>\n<li>A factor variable with many levels, even in very big data sets, may need to be consolidated into a variable with fewer levels.</li>\n<li>Some factors have levels that are ordered and can be represented as a single numeric variable.</li>\n</ul>\n<h2 id=\"interpreting-the-regression-equation\">Interpreting the Regression Equation</h2>\n<h3 id=\"key-terms-33\">Key Terms</h3>\n<ul>\n<li><p><strong>Correlated Variables</strong></p>\n<ul>\n<li>When the predictor variables are highly correlated, it is difficult to interpret the individual coefficients.</li>\n</ul></li>\n<li><p><strong>Multicollinearity</strong></p>\n<ul>\n<li>When the predictor variables have perfect, or near perfect, correlation, the regression can be unstable or impossible to compute. (Synonym: collinearity)</li>\n</ul></li>\n<li><p><strong>Confounding Variables</strong></p>\n<ul>\n<li>An important predictor that, when omitted, leads to spurious relationships in a regression equation.</li>\n</ul></li>\n<li><p><strong>Main Effects</strong></p>\n<ul>\n<li>The relationship between a predictor and the outcome variable, independent of other variables.</li>\n</ul></li>\n<li><p><strong>Interactions</strong></p>\n<ul>\n<li>An interdependent relationship between two or more predictors and the response.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-35\">Key Ideas</h3>\n<ul>\n<li>Because of correlation between predictors, care must be taken in the interpretation of the coefficients in multiple linear regression.</li>\n<li>Multicollinearity can cause numerical instability in fitting the regression equation.</li>\n<li>A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships.</li>\n<li>An interaction term between two variables is needed if the relationship between the variables and the response is interdependent.</li>\n</ul>\n<h2 id=\"regression-diagnostics\">Regression Diagnostics</h2>\n<h3 id=\"key-terms-34\">Key Terms</h3>\n<ul>\n<li><p><strong>Standardized Residuals</strong></p>\n<ul>\n<li>Residuals divided by the standard error of the residuals.</li>\n</ul></li>\n<li><p><strong>Outliers</strong></p>\n<ul>\n<li>Records (or outcome values) that are distant from the rest of the data (or the predicted outcome).</li>\n</ul></li>\n<li><p><strong>Influential Value</strong></p>\n<ul>\n<li>A value or record whose presence or absence makes a big difference in the regression equation.</li>\n</ul></li>\n<li><p><strong>Leverage</strong></p>\n<ul>\n<li>The degree of influence that a single record has on a regression equation. (Synonym: hat-value)</li>\n</ul></li>\n<li><p><strong>Non-normal residuals</strong></p>\n<ul>\n<li>Non-normally distributed residuals can invalidate some technical requirements of regression but are usually not a concern in data science.</li>\n</ul></li>\n<li><p><strong>Heteroskedasticity</strong></p>\n<ul>\n<li>When some ranges of the outcome experience residuals with higher variance (may indicate a predictor missing from the equation).</li>\n</ul></li>\n<li><p><strong>Partial Residual Plots</strong></p>\n<ul>\n<li>A diagnostic plot to illuminate the relationship between the outcome variable and a single predictor. (Synonym: added variable plot)</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-36\">Key Ideas</h3>\n<ul>\n<li>While outliers can cause problems for small data sets, the primary interest with outliers it to identify problems with the data, or locate anomalies.</li>\n<li>Single records (including regression outliers) can have a big influence on a regression equation with small data, but this effect washes out in big data.</li>\n<li>If the regression model is used for formal inference (p-values and the like), then certain assumptions about the distribution of the residuals should be checked. In general, however, the distribution of redisuals is not critical in data science.</li>\n<li>The partial residuals plot can be used to qualitatively assess the fit for each regression term, possibly leading to alternative model specifications.</li>\n</ul>\n<h2 id=\"nonlinear-regression\">Nonlinear Regression</h2>\n<h3 id=\"key-terms-35\">Key Terms</h3>\n<ul>\n<li><p><strong>Polynomial regression</strong></p>\n<ul>\n<li>Adds polynomial terms (squares, cubes, etc.) to a regression.</li>\n</ul></li>\n<li><p><strong>Spline regression</strong></p>\n<ul>\n<li>Fitting a smooth curve with a series of polynomial segments.</li>\n</ul></li>\n<li><p><strong>Knots</strong></p>\n<ul>\n<li>Values that separate spline segments.</li>\n</ul></li>\n<li><p><strong>Generalized additive models</strong></p>\n<ul>\n<li>Spline models with automated selection of knots. (Synonym: GAM)</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-37\">Key Ideas</h3>\n<ul>\n<li>Outliers in regression are records with a large residual.</li>\n<li>Multicollinearity can cause numerical instability in fitting the regression equation.</li>\n<li>A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships.</li>\n<li>An interaction term between two bariables is needed if the effect of one variable depends on the level or magnitude of the other.</li>\n<li>Polynomial regression can fit nonlinear relationships between predictos and the outcome variable.</li>\n<li>Splines are series of polynomial segments strung together, joining at knots.</li>\n<li>We can automate the process of specifying the knots in splines using generalized additive models (GAM).</li>\n</ul>\n<h1 id=\"chapter-5---classification\">Chapter 5 - Classification</h1>\n<h2 id=\"naive-bayes\">Naive Bayes</h2>\n<h3 id=\"key-terms-36\">Key Terms</h3>\n<ul>\n<li><p><strong>Conditional Probability</strong></p>\n<ul>\n<li>The probability of observing some event (say, x = i) given some other event (say, Y = i), written as P(X<sub>i</sub>|Y<sub>i</sub>)</li>\n</ul></li>\n<li><p><strong>Posterior Probability</strong></p>\n<ul>\n<li>The probability of an outcome after the predictor information has been incorporated (in contrast to the <em>prior probability</em> of outcomes, not taking predictor information into account).</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-38\">Key Ideas</h3>\n<ul>\n<li>Naive Bayes works with categorical (factor) predictors and outcomes.</li>\n<li>It asks, ‚ÄúWithin each outcome category, which predictor categories are most probable?‚Äù</li>\n<li>That information is then inverted to estimate probabilities of outcome categories, given predictor values.</li>\n</ul>\n<h2 id=\"discriminant-analysis\">Discriminant Analysis</h2>\n<h3 id=\"key-terms-37\">Key Terms</h3>\n<ul>\n<li><p><strong>Covariance</strong></p>\n<ul>\n<li>A measure of the extent to which one variable varies in concert with another (i.e., similar magnitude and direction).</li>\n</ul></li>\n<li><p><strong>Discriminant function</strong></p>\n<ul>\n<li>The function that, when applied to the predictor variables, maximizes the separation of the classes.</li>\n</ul></li>\n<li><p><strong>Discriminant weights</strong></p>\n<ul>\n<li>The scores that result from the applications of the discriminant function and are used to estimate probabilities belonging to one class or another.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-39\">Key Ideas</h3>\n<ul>\n<li>Discriminant analysis works with continuous or categorical predictors, as well as with categorical outcomes.</li>\n<li>Using the covariance matrix, it calculates a <em>linear discriminant function</em>, which is used to distinguish records belonging to one class from those belonging to another.</li>\n<li>This function is applied to the records to derive weights, or scores, for each record (one weight for each possible class), which determines its estimated class.</li>\n</ul>\n<h2 id=\"logistic-regression\">Logistic Regression</h2>\n<h3 id=\"key-terms-38\">Key Terms</h3>\n<ul>\n<li><p><strong>Logit</strong></p>\n<ul>\n<li>The function that maps class membership probability to a range from plus/minus infinity (instead of 0 to 1).</li>\n</ul></li>\n<li><p><strong>Odds</strong></p>\n<ul>\n<li>The ration of ‚Äúsuccess‚Äù (1) to ‚Äúnot success‚Äù (0).</li>\n</ul></li>\n<li><p><strong>Log odds</strong></p>\n<ul>\n<li>The response in the transformed model (now linear), which gets mapped back to a probability.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-40\">Key Ideas</h3>\n<ul>\n<li>Logistic regression is like linear regression, except that the outcomes ia a binary variable.</li>\n<li>Several transformations are needed to get the model into a form that can be fit as a linear model, with the log of the odds ration as the response variable.</li>\n<li>After the linear model is fit (by an iterative process), the log odds is mapped back to a probability.</li>\n<li>Logistic regression is popular because it is computationally fast and produces a model that can be scored to new data with only a few arithmetic operations.</li>\n</ul>\n<h2 id=\"evaluating-classification-models\">Evaluating Classification Models</h2>\n<h3 id=\"key-terms-39\">Key Terms</h3>\n<ul>\n<li><p><strong>Accuracy</strong></p>\n<ul>\n<li>The percent (or proportion) of cases classified correctly.</li>\n</ul></li>\n<li><p><strong>Confusion Matrix</strong></p>\n<ul>\n<li>A tabular display (2x2 in the binary case) of the record counts by their predicted and actual classification status.</li>\n</ul></li>\n<li><p><strong>Sensitivity</strong></p>\n<ul>\n<li>The percent (or proportion) of all 1s that are correctly classified as 1s. (Synonym: Recall)</li>\n</ul></li>\n<li><p><strong>Specificity</strong></p>\n<ul>\n<li>The percent (or proportion) of all 0s that are correctly classified as 0s.</li>\n</ul></li>\n<li><p><strong>Precision</strong></p>\n<ul>\n<li>The percent (proportion) of predicted 1s that are actually 1s.</li>\n</ul></li>\n<li><p><strong>ROC Curve</strong></p>\n<ul>\n<li>A plot of sensitivity versus specificity.</li>\n</ul></li>\n<li><p><strong>Lift</strong></p>\n<ul>\n<li>A measure of how effective the model is at identifying (comparatively rare) 1s at different probability cutoffs.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-41\">Key Ideas</h3>\n<ul>\n<li>Accuracy (the percent of predicted classifications that are correct) is but a first step in evaluating a model.</li>\n<li>Other metrics (recall, specificity, precision) focus on more specific performance characteristics (e.g., recall measures how good a model is at correctly identifying 1s).</li>\n<li>AUC (area under the ROC curve) is a common metric for the ability of a model to distinguish 1s from 0s.</li>\n<li>Similarly, lift measures how effective a model is in identifying the 1s, and it is often calculated decile by decile, starting with the most probable 1s.</li>\n</ul>\n<h2 id=\"imbalanced-data\">Imbalanced Data</h2>\n<h3 id=\"key-terms-40\">Key Terms</h3>\n<ul>\n<li><p><strong>Undersample</strong></p>\n<ul>\n<li>Use fewer of the prevalent class records in the classification model. (Synonym: Downsample)</li>\n</ul></li>\n<li><p><strong>Oversample</strong></p>\n<ul>\n<li>Use more of the rare class recprds in the classification model, bootstrapping if necessary. (Synonym: Upsample)</li>\n</ul></li>\n<li><p><strong>Up weight or down weight</strong></p>\n<ul>\n<li>Attach more (or less) weight to the rare (or prevalent) class in the model.</li>\n</ul></li>\n<li><p><strong>Data generation</strong></p>\n<ul>\n<li>Like bootstrapping, except each new bootstrapped record is slightly different from its source.</li>\n</ul></li>\n<li><p><strong>z-score</strong></p>\n<ul>\n<li>The value that results after standardization.</li>\n</ul></li>\n<li><p><strong>K</strong></p>\n<ul>\n<li>The number of neighbors considered in the nearest neighbor calculation.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-42\">Key Ideas</h3>\n<ul>\n<li>Highly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for classification algorithms.</li>\n<li>One strategy for working with imbalanced data is to balance the training data via undersampling the abundant case (or oversampling the rare case).</li>\n<li>If using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE to create synthetic data similar to existing rare cases.</li>\n<li>Imbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and that value ration should be built into the assessment metric.</li>\n</ul>\n<h1 id=\"chapter-6---statistical-machine-learning\">Chapter 6 - Statistical Machine Learning</h1>\n<h2 id=\"k-nearest-neighbors\">K-Nearest Neighbors</h2>\n<h3 id=\"key-terms-41\">Key Terms</h3>\n<ul>\n<li><p><strong>Neighbor</strong></p>\n<ul>\n<li>A record that has similar predictor values to another record.</li>\n</ul></li>\n<li><p><strong>Distance Metrics</strong></p>\n<ul>\n<li>Measures that sum up in a single number how far one record is from another.</li>\n</ul></li>\n<li><p><strong>Standardization</strong></p>\n<ul>\n<li>Subtract the mean and divide by the standard deviation. (Synonym: Normalization)</li>\n</ul></li>\n<li><p><strong>z-score</strong></p>\n<ul>\n<li>The value that results after standardization.</li>\n</ul></li>\n<li><p><strong>K</strong></p>\n<ul>\n<li>The number of neighbors considered in the nearest neighbor calculation.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-43\">Key Ideas</h3>\n<ul>\n<li>K-Nearest Neighbors (KNN) classifies a record by assigning it to the class that similar records belong to.</li>\n<li>Similarity (distance) is determined by Euclidian distance or other related metrics.</li>\n<li>The number of nearest neighbors to compare a record to, K, is determined by how well the algorithm performs on training data, using different values for K.</li>\n<li>Typically, the predictor variables are standardized so that variables of large scale do not dominate the distance metric.</li>\n<li>KNN is often used as a first stage in predictive modeling, and the <em>predictor</em> value is added back into the data as a predictor for second-stage (non-KNN) modeling</li>\n</ul>\n<h2 id=\"trees\">Trees</h2>\n<h3 id=\"key-terms-42\">Key Terms</h3>\n<ul>\n<li><p><strong>Recursive partitioning</strong></p>\n<ul>\n<li>Repeatedly dividing and subdividing the data with the goal of making the outcomes in each final subdivision as homogeneous as possible.</li>\n</ul></li>\n<li><p><strong>Split value</strong></p>\n<ul>\n<li>A predictor value that divides the records into those where that predictor is less than the split value, and those where it is more.</li>\n</ul></li>\n<li><p><strong>Node</strong></p>\n<ul>\n<li>In the decision tree, or in the set of corresponding branching rules, a node is the graphical or rule representation of a split value.</li>\n</ul></li>\n<li><p><strong>Leaf</strong></p>\n<ul>\n<li>The end of a set of if-then rules, or branches of a tree - the rules that bring you to that leaf provide one of the classification rules for any record in a tree.</li>\n</ul></li>\n<li><p><strong>Loss</strong></p>\n<ul>\n<li>The number of misclassifications at a stage in the splitting process; the more losses, the more impurity.</li>\n</ul></li>\n<li><p><strong>Impurity</strong></p>\n<ul>\n<li>The extent to which a mix of classes is found in a subpartition of the data (the more mixed, the more impure). (Synonym: Heterogeneity) (Antonyms: Homogeneity, purity)</li>\n</ul></li>\n<li><p><strong>Pruning</strong></p>\n<ul>\n<li>The process of taking a fully grown tree and progressively cutting its branches back to reduce overfitting.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-44\">Key Ideas</h3>\n<ul>\n<li>Decision trees produce a set of rules to classify or predict an outcome.</li>\n<li>The rules correspond to successive partitioning of the data into subpartitions.</li>\n<li>Each partition, or split, references a specific value of a predictor variable and divides the data into records where that predictor value is above or below that split value.</li>\n<li>At each stage, the tree algorithm chooses the split that minimizes the outcome impurity within each subpartition.</li>\n<li>When no further splits can be made, the tree is fully grown and each terminal node, or leaf, has records of a single class; new cases folling that rule (split) path would be assigned that class.</li>\n<li>A fully grown tree overfits data and must be pruned back so that it captures signal and not noise.</li>\n<li>Multiple-tree algorithms like random forests and boosted trees yield better predictive performance, but they lose the rule-based communicative power of single trees</li>\n</ul>\n<h2 id=\"bagging-and-the-random-forst\">Bagging and the Random Forst</h2>\n<h3 id=\"key-terms-43\">Key Terms</h3>\n<ul>\n<li><p><strong>Ensemble</strong></p>\n<ul>\n<li>Forming a prediction by using a collection of models. (Synonym: Model averaging)</li>\n</ul></li>\n<li><p><strong>Bagging</strong></p>\n<ul>\n<li>A general technique to form a collection of models by bootstrapping the data. (Synonym: Bootstrap aggregation)</li>\n</ul></li>\n<li><p><strong>Random Forest</strong></p>\n<ul>\n<li>A type of bagged estimate based on decision tree models. (Synonym: Bagged decision trees)</li>\n</ul></li>\n<li><p><strong>Variable Importance</strong></p>\n<ul>\n<li>A measure of the importance of a precitor variable in the performance of the model.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-45\">Key Ideas</h3>\n<ul>\n<li>Ensemble models improve model accuracy by combining the results from many models.</li>\n<li>Bagging is a particular type of ensemble model based on fitting many models to bootstrapped samples of the data and averaging the models.</li>\n<li>Random forest is a special type of bagging applied to decision trees. In addition to resampling the data, the random forest algorithm samples the predictor variables when splitting the trees.</li>\n<li>A useful output from the random forest is a measure of variable importance that ranks the predictors in terms of their contribution to model accuracy.</li>\n<li>The random forest has a set of hyperparameters that should be tuned using cross-validation to avoid overfitting.</li>\n</ul>\n<h2 id=\"boosting\">Boosting</h2>\n<h3 id=\"key-terms-44\">Key Terms</h3>\n<ul>\n<li><p><strong>Ensemble</strong></p>\n<ul>\n<li>Forming a prediction by using a collection of models. (Synonym: Model averaging)</li>\n</ul></li>\n<li><p><strong>Boosting</strong></p>\n<ul>\n<li>A general technique to fit a sequence of models by giving more weight to the records with large residuals for each successive round.</li>\n</ul></li>\n<li><p><strong>Adaboost</strong></p>\n<ul>\n<li>An early version of boosting that reqeights the data based on the residuals.</li>\n</ul></li>\n<li><p><strong>Gradient Boosting</strong></p>\n<ul>\n<li>A more general form of boosting that is cast in terms of minimizing a cost function.</li>\n</ul></li>\n<li><p><strong>Stochastic Gradient Boosting</strong></p>\n<ul>\n<li>The most general algorithm for boosting that incorporates resampling of records and columns in each round.</li>\n</ul></li>\n<li><p><strong>Regularization</strong></p>\n<ul>\n<li>A technique to avoid overfitting by adding a penalty term to the cost function on the number of parameters in the model.</li>\n</ul></li>\n<li><p><strong>Hyperparameters</strong></p>\n<ul>\n<li>Parameters that need to be set before fitting the algorithm.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-46\">Key Ideas</h3>\n<ul>\n<li>Boosting is a class of ensemble models based on fitting a sequence of models, with more weight given to records with large errors in successive rounds.</li>\n<li>Stochastic gradient boosting is the most general type of boosting and offers the best performance. The most common form of stochastic gradient boosting uses tree models.</li>\n<li>XGBoost is a popular and computationally efficient software package for stochastic gradient boosting: it is available is all commong languages used in data science.</li>\n<li>Boosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.</li>\n<li>Regularization is one way to avoid overfitting by including a penalty term on the number of parameters (e.g., tree size) in a model.</li>\n<li>Cross-validation is especially important for boosting due to the large number of hyperparameters that need to be set.</li>\n</ul>\n<h1 id=\"chapter-7---unsupervised-learning\">Chapter 7 - Unsupervised Learning</h1>\n<h2 id=\"principal-component-analysis\">Principal Component Analysis</h2>\n<h3 id=\"key-terms-45\">Key Terms</h3>\n<ul>\n<li><p><strong>Principal Component</strong></p>\n<ul>\n<li>A linear combination of the predictor variables.</li>\n</ul></li>\n<li><p><strong>Loadings</strong></p>\n<ul>\n<li>The weights that transform the predictors into the components. (Synonym: Weights)</li>\n</ul></li>\n<li><p><strong>Screeplot</strong></p>\n<ul>\n<li>A plot of the variances of the components, showing the relative importance of the components, either as explained variance or as proportion of explained variance.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-47\">Key Ideas</h3>\n<ul>\n<li>Principal components are linear combinations of the predictor variables (numeric data only).</li>\n<li>Principal components are calculated so as to minimize correlation between components, reducing redundancy.</li>\n<li>A limited number of components will typically explain most of the variance in the outcome variable.</li>\n<li>The limited set of principal components can then be used in place of the (more numerous) original predictors, reducing dimensionality.</li>\n<li>A superficially similar technique for categorical data is correspondence analysis, but it is not useful in a big data context.</li>\n</ul>\n<h2 id=\"k-means-clustering\">K-Means Clustering</h2>\n<h3 id=\"key-terms-46\">Key Terms</h3>\n<ul>\n<li><p><strong>Cluster</strong></p>\n<ul>\n<li>A group of records that are similar.</li>\n</ul></li>\n<li><p><strong>Cluster mean</strong></p>\n<ul>\n<li>The vector of variable means for the records in a cluster.</li>\n</ul></li>\n<li><p><strong>K</strong></p>\n<ul>\n<li>The nuber of clusters.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-48\">Key Ideas</h3>\n<ul>\n<li>The number of desired clusters, K, is chosen by the user.</li>\n<li>The algorithm develops clusters by iteratively assigning records to the nearest cluster mean until clust assignments do not chance.</li>\n<li>Practical considerations usually dominate the choice of K; there is no statistically determined optimal number of clusters.</li>\n</ul>\n<h2 id=\"hierarchical-clustering\">Hierarchical Clustering</h2>\n<h3 id=\"key-terms-47\">Key Terms</h3>\n<ul>\n<li><p><strong>Dendrogram</strong></p>\n<ul>\n<li>A visual representation of the records and the hierarchy of clusters to which they belong.</li>\n</ul></li>\n<li><p><strong>Distance</strong></p>\n<ul>\n<li>A measure of how close one record is to another.</li>\n</ul></li>\n<li><p><strong>Dissimilarity</strong></p>\n<ul>\n<li>A measure of how close one cluster is to another.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-49\">Key Ideas</h3>\n<ul>\n<li>Hierarchical clustering starts with every record in its own cluster.</li>\n<li>Progressively, clusters are joined to nerby clusters until all records belong to a single cluster (the agglomerative algorithm).</li>\n<li>The agglomeration history is retained and plotted, and the user (without specifying the number of clusters beforehand) can visualize the number and the structure of clusters at different stages.</li>\n<li>Inter-cluster distances are computed in different ways, all relying on the set of all inter-record distances.</li>\n</ul>\n<h2 id=\"model-based-clustering\">Model-Based Clustering</h2>\n<h3 id=\"key-ideas-50\">Key Ideas</h3>\n<ul>\n<li>Clusters are assumed to derive from different data-generating processes with different probability distributions.</li>\n<li>Different models are fit, assuming different numbers of (typically normal) distributions.</li>\n<li>The method chooses the model (and the associated number of clusters) that fits the data well without using too many parameters (i.e., overfitting).</li>\n</ul>\n<h2 id=\"scaling-data\">Scaling Data</h2>\n<h3 id=\"key-terms-48\">Key Terms</h3>\n<ul>\n<li><p><strong>Scaling</strong></p>\n<ul>\n<li>Squashing or expanding data, usually to bring multiple variables to the same scale.</li>\n</ul></li>\n<li><p><strong>Normalization</strong></p>\n<ul>\n<li>One method of scaling - subtracting the mean and dividing by the standard deviation. (Synonym: Standardization)</li>\n</ul></li>\n<li><p><strong>Gower‚Äôs Distance</strong></p>\n<ul>\n<li>A scaling algorithm applied to mixed numeric and categorical data to bring all variables to a 0-1 range.</li>\n</ul></li>\n</ul>\n<h3 id=\"key-ideas-51\">Key Ideas</h3>\n<ul>\n<li>Variables measured on different scaled need to be transformed to similar scales so that their impact on algorithms is not determined mainly by their scale.</li>\n<li>A common scaling method is normalization (standardization) - subtracting the mean and dividing by the standard deviation.</li>\n<li>Another method is Gower‚Äôs distance, which scales all variables to the 0-1 range (it is often used with mixed numeric and categorical data).</li>\n</ul>\n"
  },
  {
    "_id": "design_patterns",
    "_body": "<h1 id=\"design-principles\">Design Principles</h1>\n<ul>\n<li><h3 id=\"encapsulate-what-varies\">Encapsulate What Varies</h3>\n<p>Identify the aspects of your application that vary and separate them from what stays the same. Take what varies and ‚Äúencapsulate‚Äù it so it won‚Äôt affect the rest of your code. This results in fewer unintended consequences from code changes and more flexibility in the system.</p></li>\n<li><h3 id=\"program-to-an-interface-not-an-implementation\">Program to an Interface, NOT an Implementation</h3></li>\n<li><h3 id=\"favor-composition-over-inheritance\">Favor Composition over Inheritance</h3></li>\n<li><h3 id=\"loose-coupling\">Loose Coupling</h3>\n<p>Strive for loosely coupled designes between objects that interact. Loosely coupled designes allow us to build flexible systems that can handle change because they minimize the interdependency between objects.</p></li>\n<li><h3 id=\"open-closed-principle\">Open-Closed Principle</h3>\n<p>Class should be open for extension, but closed for modification. The goal is to allow classes to be easily extended to incorporate new behavior without modifying existing code. What do we get if we accomplish this? Designes that are resilient to change and flexible enough to take on new functionality to meet changing requirements. Be careful when choosing the areas of code that need to be extended: applying the Open-Closed Principle everywhere is wasteful and unnecessary, and can lead to complex, hard-to-understand code.</p></li>\n<li><h3 id=\"dependency-inversion-principle\">Dependency Inversion Principle</h3>\n<p>Depend upon abstractions. Do not depend upon concrete classes. High-level components should not depend on low-level components; rather, they should both depend on abstractions.</p></li>\n<li><h3 id=\"principle-of-least-knowledge\">Principle of Least Knowledge</h3>\n<p>Talk only to your immediate friends. When you are designing a system, be careful of the number of classes it interacts with and also how it comes to interact with those classes. This prevents us from creating designs that have a large number of classes coupled together so that changes in one part of the system cascade to other parts, When you build a lot of dependencoes between many classes, you are building a fragile system that will be costly to maintain and complicated for others to understand. The principle provides some guidelines (Law of Demeter): take any object; now from any method in that object, the principle tells us that we should only invoke methods that belong to:</p>\n<ul>\n<li>The object itself</li>\n<li>Objects passed in as a parameter to the method</li>\n<li>Any object the method creates or instantiates</li>\n<li>Any components of the object</li>\n</ul></li>\n<li><h3 id=\"the-hollywood-principle\">The Hollywood Principle</h3>\n<p>‚ÄúDon‚Äôt call us, we‚Äôll call you‚Äù The Hollywood Principle gives us a way to prevent ‚Äúdependency rot.‚Äù Dependency rot happens when you have high-level components depending on low-level components depending on high-level components depending on side-ways components depending on low-level components, and so on. When rot sets in, no one can easily understand the way a system is designed. With the Hollywood principle, we allow low-level components to hook themselves into a system, but the high-level components determine when they are needed, and how. In other words, the high-level components give the low-level components a ‚Äúdon‚Äôt call us, we‚Äôll call you‚Äù treatment.</p></li>\n<li><h3 id=\"single-responsibility-principle\">Single Responsibility Principle</h3>\n<p>A class should have only one reason to change. Every responsibility of a class is an area of potential change, More than one responsibility means more than one area of change. This principle guides us to keep each class to a single responsibility.</p></li>\n</ul>\n<h1 id=\"design-patterns\">Design Patterns</h1>\n<p>NOTE: Code examples and classes that are referenced are found <a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns\">here</a>.</p>\n<h2 id=\"creational-patterns\">Creational Patterns</h2>\n<hr />\n<h3 id=\"simple-factory\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/simplefactory\">Simple Factory</a></strong></h3>\n<p>Simple Factory isn‚Äôt an actual design pattern, more of a programming idiom that is commonly used.</p>\n<h3 id=\"abstract-factory\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/abstractfactory\">Abstract Factory</a></strong></h3>\n<p>The Abstract Factory Pattern provides an interface for creating families of related or dependent objects without specifying their concrete classes.</p>\n<h3 id=\"factory-method\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/factorymethod\">Factory Method</a></strong></h3>\n<p>The Factory Method Pattern defines an interface for creating an object, but lets subclasses decide which clas to instantiate. Factory method lets a class defer instantiation to subclasses.</p>\n<div class=\"sourceCode\" id=\"cb1\"><pre class=\"sourceCode java\"><code class=\"sourceCode java\"><span id=\"cb1-1\"><a href=\"#cb1-1\" aria-hidden=\"true\" tabindex=\"-1\"></a><span class=\"kw\">abstract</span> Product <span class=\"fu\">factoryMethod</span><span class=\"op\">(</span><span class=\"bu\">String</span> type<span class=\"op\">)</span></span></code></pre></div>\n<h3 id=\"singleton\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/singleton\">Singleton</a></strong></h3>\n<p>The Singleton Pattern ensures a class has only one instannce, and provides a global point of access to it.</p>\n<p>The <code>getInstance()</code> method is static, which means it‚Äôs a class method, so you can conveniently access this method from anywhere in your code using <code>Singleton.getInstance()</code>. That‚Äôs just as easy as accessing a global variable, but we get benefits like lazy initialization from the Singleton. The <code>uniqueInstance</code> class variable holds our one and only instance of Singleton. A class implementing the Singleton pattern is more than a Singleton; it is a general purpose class with its own set of data and methods.</p>\n<h4 id=\"handling-multithreading-issues\">Handling Multithreading Issues</h4>\n<p><em>SynchronizedSingleton</em> - By adding the synchronized keyword to <code>getInstance()</code>, we force every thread to wait its turn before it can enter the method. That is, notwo threads may enter the method at the same time. Keep in mind that synchronizing a method can decrease performance by a factor of 100, so if a high-traffic part of your code begins using <code>getInstance()</code>, another option might have to be considered.</p>\n<p><em>EagerSingleton</em> - If your application always creates and uses an instance of the Singleton or the overhead of creation and runtime aspects of the Singleton are not onerous, you may want to create your Singleton eagerly.</p>\n<p><em>DoubleCheckedLockingSingleton</em> - With double-checked locking, we first check to see if an instance is created, and if not, THEN we synchronize. This way, we only synchronize the first time through. The <code>volatile</code> keyword ensure that multiple threads handle the <code>uniqueInstance</code> variable correctly when it is being initialized to the Singleton instance</p>\n<h2 id=\"structural-patterns\">Structural Patterns</h2>\n<hr />\n<h3 id=\"adapter\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/adapter\">Adapter</a></strong></h3>\n<p>The Adapter Pattern converts the interface of a class into another interface the clients expect. Adapter lets classes work together that couldn‚Äôt otherwise because of incompatible interfaces.</p>\n<h3 id=\"composite\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/composite\">Composite</a></strong></h3>\n<p>The Composite Pattern allows you to compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly. Using a composite structures, we can apply the same operations over both composites and individual objects. In other words, in most cases, we can ignore the differences between compositions of objects and individual objects.</p>\n<h3 id=\"decorator\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/decorator\">Decorator</a></strong></h3>\n<p>The Decorator Pattern attaches additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality.</p>\n<p>The java.io package heavily uses the decorator design pattern.</p>\n<p>Each decorator HAS-A (wraps) a components, which means the decorator has an instance variable that holds a reference to a components. Decorators implement the same interface or abstract class as the component they are going to decorate.</p>\n<h3 id=\"facade\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/facade\">Facade</a></strong></h3>\n<p>The Facade Pattern provides a unified interface to a set of interfaces in a subsystem. Facade defines a higher-level interface that makes the subsystem easier to use.</p>\n<h2 id=\"behavioral-patterns\">Behavioral Patterns</h2>\n<hr />\n<h3 id=\"command\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/command\">Command</a></strong></h3>\n<p>The Command pattern encapsulates a request as an object, thereby letting you parameterize other objects with different requests, queue or log requests, and support undoable operations.</p>\n<p>Command declares an interface for all commands. A command is invoked through its <code>execute()</code> method, which asks a receiver to perform an action. It can also perform <code>undo()</code> actions.</p>\n<h3 id=\"iterator\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/iterator\">Iterator</a></strong></h3>\n<p>The Iterator Pattern provides a way to access the elements of an aggregate object sequentially without exposing its underlying representation. It also places the task of traversal on the iterator object, not on the aggregate, which simplifies the aggregate interface and implementation, and places the responsibility where it should be.</p>\n<p>The Iterator interface provides the interface that all iterators must implement, and a set of methods for traversing over elements of a collection. Here we‚Äôre using the java.util.Iterator. If you don‚Äôt want to use Java‚Äôs Iterator interface, you can always create your own.</p>\n<h3 id=\"observer\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/observer\">Observer</a></strong></h3>\n<p>The Oberver Pattern defines a one-to-many dependency between objects so that when one object changes state, all of its dependets are notified and updated automatically.</p>\n<p>Objects use the Subject interface to register as observers and also to remove themselves from being observers. Each Subject can have many observers.</p>\n<p>A concrete subject always implements the Subject interface. In addition to the register and remove methods, the concrete subject implements a notifyObservers() method that is used to updateall the current observers whenever state changes. The concrete subject may also have mthods for setting and getting its state.</p>\n<p>All potential observers need to implement the Observer interface. This interface just has one method, update(), that gets called when the Subject‚Äôs state changes.</p>\n<p>Concrete observers can be any class that implements the Observer interface. Each observer registers with a concrete subject to recieve updates.</p>\n<h3 id=\"state\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/state\">State</a></strong></h3>\n<p>The State Pattern allows an object to alter its behavior when its internal state changes. The object will appear to change its class.</p>\n<p>The Context is the class that cna have a number of internal states. In our example, the GumballMachine is the Context. Whenever the request() is made on the Context it is delegated to the state to hande.</p>\n<p>The State interface defines a common interface for all concrete states; the states all implement the same interface so they are interchangeable. State</p>\n<p>ConcreteStates handle requests from the Context. Each ConcreteState provides its own implementation for a request. In this way, when the Context changes state, its behavior will change as well</p>\n<p>Many concrete states are possible</p>\n<h3 id=\"strategy\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/strategy\">Strategy</a></strong></h3>\n<p>The Strategy pattern defines a family of algorithms, encapsulates each one, and makes them interchangeable. Strategy lets the algorithm vary indepentently from clients that use it.</p>\n<h3 id=\"template-method\"><strong><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/templatemethod\">Template Method</a></strong></h3>\n<p>The Template Method Pattern defines the skelwton of an algorithm in a method, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm‚Äôs structure</p>\n<p>The AbstractClass contains the template method and abstract versions of the operations used in the template method. The template method makes use of the primitiveOperations to implement an algorithm. It is decoupled from the actual implementation of these operations.</p>\n<p>There may be many ConcreteClasses, each implementing the full set of operations required by the template method. The ConcreteClass implements the abstract operations, which are called when the templateMethod() needs them.</p>\n<p>A hook is a method that is declared in the abstract class, but only given an empty or default implementation. This gives subclasses the ability to ‚Äúhook into‚Äù the algorithm at various points, if they wish; a subclass is also free to ignore the hook.</p>\n<h2 id=\"compound---a-collection-of-design-patterns-used-together\"><a href=\"https://github.com/brombaut/BEC/tree/master/design-patterns/compound\">Compound</a> - A collection of design patterns used together</h2>\n<hr />\n<p><em>What did we do?</em></p>\n<p><strong>We started with a bunch of Quackables‚Ä¶</strong></p>\n<p><strong>A goose came along and wanted to act like a Quackable too.</strong> So we used the <em>Adapter Pattern</em> to adapt the goose to a Quackable. Now, you can call quack() on a goose wrapped in the adapter and it will honk!</p>\n<p><strong>Then, the Quackologists decided they wanred to count quacks.</strong> So we used the <em>Decorator Pattern</em> to add a QuackCounter decorator that keeps track of the number of times quack() is called, and then delegates the quack to the Quackable it‚Äôs wrapping.</p>\n<p><strong>But the Quackologists were worried they‚Äôd forget to add the QuackCounter decorator.</strong> So we used the <em>Abstract Factory Pattern</em> to create ducks for them. Now, whenever they want a duck, they ask the factory for one, and it hands back a decorated duck. (And don‚Äôt forget, they can also use another duck factory if they want an undecorated duck.)</p>\n<p><strong>We had management problems keeping track of all those ducks and geese and quackables.</strong> So we used the <em>Composite Pattern</em> to group Quackables into Flocks. The pattern also allows the Quackologist to create sub-flocks to manage duck families. We used the <em>Iterator Pattern</em> in our implementation by using java.util‚Äôs iterator in ArrayList.</p>\n<p><strong>The Quackologists also wanted to be notified when any Quackable quacked.</strong> So we used the <em>Observer Pattern</em> to let the Quackologists register as Quackable Observers. Now they‚Äôre notified every time any Quackable quacks. We used iterator again in this implementation. The Quackologists can even use the Observer Pattern with their composites.</p>\n"
  }
]